<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.1.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="江湖の林九">
<meta property="og:url" content="http://yoursite.com/page/10/index.html">
<meta property="og:site_name" content="江湖の林九">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Leo Lin">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/page/10/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>江湖の林九</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">江湖の林九</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">相逢何必曾相识！</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/20/BloomFilter%E5%8E%9F%E7%90%86%E5%8F%8A%E5%BA%94%E7%94%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Leo Lin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="江湖の林九">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/09/20/BloomFilter%E5%8E%9F%E7%90%86%E5%8F%8A%E5%BA%94%E7%94%A8/" class="post-title-link" itemprop="url">BloomFilter原理及应用</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-09-20 11:35:00" itemprop="dateCreated datePublished" datetime="2019-09-20T11:35:00+08:00">2019-09-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-04-24 14:57:44" itemprop="dateModified" datetime="2020-04-24T14:57:44+08:00">2020-04-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/" itemprop="url" rel="index"><span itemprop="name">网络爬虫</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Bloom-Filter算法"><a href="#Bloom-Filter算法" class="headerlink" title="Bloom Filter算法"></a>Bloom Filter算法</h2><p>在Bloom Filte中使用位数组辅助实现检测判断。</p>
<p>1、声明一个包含m位且所有位为0的位数组和待检测集合元素n。</p>
<p>2、用k个相互独立的哈希函数将每个元素映射到m的位数组上，散列函数得到的结果记作位置索引并将该位置置1，如果一个位置多次被置为1，那么只有第一次会起作用。</p>
<p>3、若得到新元素则判断其结果对应位置是否均为1，若有一个位置不为1，则该元素不属于该集合。<br><img src="https://i.loli.net/2019/09/09/JsTu5ofljmYEGn8.png"></p>
<p><strong>注意:</strong><br>散列函数又称散列算法、哈希函数，是一种从任何一种数据中创建小的数字“指纹”的方法。<br>散列函数把消息或数据压缩成摘要，使得数据量变小，将数据的格式固定下来。</p>
<p><strong>满足关系</strong>：m&gt;nk</p>
<p><strong>误判</strong>：<br>由于可能出现元素被映射后的位上已经被置1，导致不存在的字符串有一定概率被误判为已存在，但已存在绝对是已经存在。<br>可以增加多个bitmap，关联不同的hash函数，取否定的交集来提高bloom filter的准确率。</p>
<p><strong>数学分析得出结论</strong>：<br>最优 K =(ln2)<em>(m/n) ≈ 0.7</em>m/n , 在已知n的情况下确定m与k，若限定错误率为a，则m &gt;= nlog_2(1/a)*log_2e</p>
<h2 id="基于Redis的代码实现"><a href="#基于Redis的代码实现" class="headerlink" title="基于Redis的代码实现"></a>基于Redis的代码实现</h2><p>Redis的BitMap最大支持512MB即2^32，可用于约40亿的数据。如果去重的数据量大，需要申请多个去重块。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> redis <span class="keyword">import</span> StrictRedis</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HashMap</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, m, seed</span>):</span></span><br><span class="line">        self.m = m</span><br><span class="line">        self.seed = seed</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hash</span>(<span class="params">self, value</span>):</span></span><br><span class="line">        ret = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(value)):</span><br><span class="line">            ret += self.seed * ret + ord(value[i])</span><br><span class="line">        <span class="keyword">return</span> (self.m - <span class="number">1</span>) &amp; ret</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">BLOOMFILTER_HASH_NUMBER = <span class="number">6</span></span><br><span class="line">BLOOMFILTER_BIT = <span class="number">30</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BloomFilter</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, server, key, bit=BLOOMFILTER_BIT, hash_number=BLOOMFILTER_HASH_NUMBER</span>):</span></span><br><span class="line">        self.m = <span class="number">1</span> &lt;&lt; bit</span><br><span class="line"></span><br><span class="line">        self.seeds = range(hash_number)</span><br><span class="line"></span><br><span class="line">        self.maps = [HashMap(self.m, seed) <span class="keyword">for</span> seed <span class="keyword">in</span> self.seeds]</span><br><span class="line"></span><br><span class="line">        self.server = server</span><br><span class="line"></span><br><span class="line">        self.key = key</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">exists</span>(<span class="params">self, value</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> value:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        exist = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> map <span class="keyword">in</span> self.maps:</span><br><span class="line">            offset = map.hash(value)</span><br><span class="line">            exist = exist &amp; self.server.getbit(self.key, offset)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> exist</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert</span>(<span class="params">self, value</span>):</span></span><br><span class="line">        <span class="keyword">for</span> f <span class="keyword">in</span> self.maps:</span><br><span class="line">            offset = f.hash(value)</span><br><span class="line">            self.server.setbit(self.key, offset, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    conn = StrictRedis(host=<span class="string">&quot;localhost&quot;</span>, port=<span class="number">6379</span>)</span><br><span class="line">    bf = BloomFilter(conn, <span class="string">&#x27;testbf&#x27;</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line">    <span class="keyword">if</span> bf.exists(<span class="string">&#x27;http://www.baidu.com&#x27;</span>):  <span class="comment"># 判断字符串是否存在</span></span><br><span class="line">        print(<span class="string">&#x27;exists!&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">&#x27;not exists!&#x27;</span>)</span><br><span class="line">        bf.insert(<span class="string">&#x27;http://www.baidu.com&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="Github开源库"><a href="#Github开源库" class="headerlink" title="Github开源库"></a>Github开源库</h2><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&gt;&gt;&gt;</span> <span class="string">import</span> <span class="string">pybloom_live</span></span><br><span class="line"><span class="string">&gt;&gt;&gt;</span> <span class="string">f</span> <span class="string">=</span> <span class="string">pybloom_live.BloomFilter(capacity=1000,</span> <span class="string">error_rate=0.001)</span></span><br><span class="line"><span class="string">&gt;&gt;&gt;</span> [<span class="string">f.add(x)</span> <span class="string">for</span> <span class="string">x</span> <span class="string">in</span> <span class="string">range(10)</span>]</span><br><span class="line">[<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>]</span><br><span class="line"><span class="string">&gt;&gt;&gt;</span> <span class="string">all([(x</span> <span class="string">in</span> <span class="string">f)</span> <span class="string">for</span> <span class="string">x</span> <span class="string">in</span> <span class="string">range(10)])</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="string">&gt;&gt;&gt;</span> <span class="number">10</span> <span class="string">in</span> <span class="string">f</span></span><br><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="string">&gt;&gt;&gt;</span> <span class="number">5</span> <span class="string">in</span> <span class="string">f</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="string">&gt;&gt;&gt;</span> <span class="string">f</span> <span class="string">=</span> <span class="string">pybloom_live.BloomFilter(capacity=1000,</span> <span class="string">error_rate=0.001)</span></span><br><span class="line"><span class="string">&gt;&gt;&gt;</span> <span class="string">for</span> <span class="string">i</span> <span class="string">in</span> <span class="string">xrange(0,</span> <span class="string">f.capacity):</span></span><br><span class="line"><span class="string">...</span>     <span class="string">_</span> <span class="string">=</span> <span class="string">f.add(i)</span></span><br><span class="line"><span class="string">&gt;&gt;&gt;</span> <span class="string">(1.0</span> <span class="bullet">-</span> <span class="string">(len(f)</span> <span class="string">/</span> <span class="string">float(f.capacity)))</span> <span class="string">&lt;=</span> <span class="string">f.error_rate</span> <span class="string">+</span> <span class="number">2e-18</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="string">&gt;&gt;&gt;</span> <span class="string">sbf</span> <span class="string">=</span> <span class="string">pybloom_live.ScalableBloomFilter(mode=pybloom_live.ScalableBloomFilter.SMALL_SET_GROWTH)</span></span><br><span class="line"><span class="string">&gt;&gt;&gt;</span> <span class="string">count</span> <span class="string">=</span> <span class="number">10000</span></span><br><span class="line"><span class="string">&gt;&gt;&gt;</span> <span class="string">for</span> <span class="string">i</span> <span class="string">in</span> <span class="string">range(0,</span> <span class="string">count):</span></span><br><span class="line">        <span class="string">_</span> <span class="string">=</span> <span class="string">sbf.add(i)</span></span><br><span class="line"></span><br><span class="line"><span class="string">&gt;&gt;&gt;</span> <span class="string">(1.0</span> <span class="bullet">-</span> <span class="string">(len(sbf)</span> <span class="string">/</span> <span class="string">float(count)))</span> <span class="string">&lt;=</span> <span class="string">sbf.error_rate</span> <span class="string">+</span> <span class="number">2e-18</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="comment"># len(sbf) may not equal the entire input length. 0.01% error is well</span></span><br><span class="line"><span class="comment"># below the default 0.1% error threshold. As the capacity goes up, the</span></span><br><span class="line"><span class="comment"># error will approach 0.1%.</span></span><br></pre></td></tr></table></figure>
<p>推荐阅读:<a target="_blank" rel="noopener" href="https://redislabs.com/blog/rebloom-bloom-filter-datatype-redis/">Here</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/08/Scrapy%20CrawlerSpider%20Itemloader/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Leo Lin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="江湖の林九">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/08/08/Scrapy%20CrawlerSpider%20Itemloader/" class="post-title-link" itemprop="url">Scrapy CrawlerSpider ItemLoader</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-08-08 08:00:00" itemprop="dateCreated datePublished" datetime="2019-08-08T08:00:00+08:00">2019-08-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-04-23 22:19:00" itemprop="dateModified" datetime="2020-04-23T22:19:00+08:00">2020-04-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/" itemprop="url" rel="index"><span itemprop="name">网络爬虫</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="CrawlSpider"><a href="#CrawlSpider" class="headerlink" title="CrawlSpider"></a>CrawlSpider</h2><p>CrawlSpider基于ExtractorLink制定了跟进url的规则，打算从网页中获得url后继续爬取的场景非常适用。<br>1、新建项目</p>
<figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">scrapy </span>startproject XXX</span><br><span class="line"><span class="keyword">scrapy </span>genspider -t crawl xxx xxx.com</span><br></pre></td></tr></table></figure>

<p>2、 新属性</p>
<ul>
<li>rules： Rule对象的集合，用于匹配目标网站并排除干扰</li>
<li>parse_start_url：用于爬取起始响应，必须要返回Item，Request中的一个。</li>
</ul>
<p>3、LinkExtractors</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LinkExtractor(allow=(), deny=(), allow_domains=(), deny_domains=(), <span class="attribute">deny_extensions</span>=None, restrict_xpaths=(), restrict_css=(), tags=(<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;area&#x27;</span>), attrs=(<span class="string">&#x27;href&#x27;</span>, ), <span class="attribute">canonicalize</span>=<span class="literal">False</span>, <span class="attribute">unique</span>=<span class="literal">True</span>, <span class="attribute">process_value</span>=None, <span class="attribute">strip</span>=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>allow (a regular expression (or list of)) – 必须要匹配这个正则表达式(或正则表达式列表)的URL才会被提取｡如果没有给出(或为空), 它会匹配所有的链接｡</li>
<li>deny (a regular expression (or list of)) – 与这个正则表达式(或正则表达式列表)的(绝对)不匹配的URL必须被排除在外(即不提取)｡它的优先级高于 allow 的参数｡如果没有给出(或None), 将不排除任何链接｡</li>
<li>allow_domains (str or list) – 单值或者包含字符串域的列表表示会被提取的链接的domains｡</li>
<li>deny_domains (str or list) – 单值或包含域名的字符串,将不考虑提取链接的domains｡</li>
<li>deny_extensions (list) – 应提取链接时,可以忽略扩展名的列表｡如果没有给出, 它会默认为 scrapy.linkextractor 模块中定义的 IGNORED_EXTENSIONS 列表｡</li>
<li>restrict_xpaths (str or list) – 一个的XPath (或XPath的列表),它定义了链路应该从提取的响应内的区域｡</li>
<li>tags (str or list) – 提取链接时要考虑的标记或标记列表｡默认为 ( ‘a’ , ‘area’) ｡</li>
<li>attrs (list) – 提取链接时应该寻找的attrbitues列表(仅在 tag 参数中指定的标签)｡默认为 (‘href’)｡</li>
<li>canonicalize (boolean) – 规范化每次提取的URL(使用scrapy.utils.url.canonicalize_url )｡默认为 True ｡</li>
<li>unique (boolean) – 重复过滤是否应适用于提取的链接｡</li>
<li>process_value (callable) – 见:class:BaseSgmlLinkExtractor 类的构造函数 process_value 参数｡</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">pattern = <span class="string">r&#x27;/intro/\w+$&#x27;</span></span><br><span class="line">link_extractor = LinkExtractor(deny = pattern)</span><br><span class="line">links = link_extractor.extract_links(response)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_start_url</span>(<span class="params">self, response</span>):</span></span><br><span class="line">	self.logger.info(<span class="string">&#x27;parse_start_url %s&#x27;</span>, response.url)</span><br><span class="line">	next_page = response.css(<span class="string">&#x27;li.next a::attr(&quot;href&quot;)&#x27;</span>).extract_first()</span><br><span class="line">	<span class="keyword">if</span> next_page <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">		<span class="keyword">yield</span> response.follow(next_page, self.next_parse)</span><br></pre></td></tr></table></figure>
<p>4、Rule<br>在rules中包含一个或多个Rule对象，每个Rule对爬取网站的动作定义了特定操作，CrawlSpider使用parse方法来实现其逻辑，应用时避免用parse作为回调函数。</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> scrapy.spiders.<span class="constructor">Rule(<span class="params">link_extractor</span>, <span class="params">callback</span> = None, <span class="params">cb_kwargs</span> = None, <span class="params">follow</span> = None, <span class="params">process_links</span> = None, <span class="params">process_request</span> = None)</span></span><br></pre></td></tr></table></figure>
<ul>
<li>callback： 从link_extractor中每获取到链接时，参数所指定的值作为回调函数，该回调函数接受一个response作为其第一个参数。 注意：当编写爬虫规则时，避免使用parse作为回调函数。由于CrawlSpider使用parse方法来实现其逻辑，如果覆盖了 parse方法，crawl spider将会运行失败。</li>
<li>follow：是一个布尔(boolean)值，指定了根据该规则从response提取的链接是否需要跟进。 如果callback为None，follow 默认设置为True ，否则默认为False。</li>
<li>process_links：指定该spider中哪个的函数将会被调用，从link_extractor中获取到链接列表时将会调用该函数。</li>
<li>process_request：指定该spider中哪个的函数将会被调用， 该规则提取到每个request时都会调用该函数。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">rules = (</span><br><span class="line">	Rule(LinkExtractor(allow=<span class="string">r&#x27;/subject/\d+/$&#x27;</span>), callback=<span class="string">&#x27;parse_item&#x27;</span>, follow=<span class="literal">True</span>, process_request=<span class="string">&#x27;get_cookie&#x27;</span>),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_cookie</span>(<span class="params">self, request</span>):</span></span><br><span class="line">	bid = <span class="string">&#x27;&#x27;</span>.join(random.choice(string.ascii_letters + string.digits) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">11</span>))</span><br><span class="line">	request.cookies[<span class="string">&#x27;bid&#x27;</span>] = bid</span><br><span class="line">	<span class="keyword">return</span> request</span><br></pre></td></tr></table></figure>

<h2 id="Itemloader"><a href="#Itemloader" class="headerlink" title="Itemloader"></a>Itemloader</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    演示填充数据的三种方法</span></span><br><span class="line"><span class="string">    name字段值是两个xpath累加后得到的</span></span><br><span class="line"><span class="string">    :param self:</span></span><br><span class="line"><span class="string">    :param response:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    l = ItemLoader(item=Product(), response=response)</span><br><span class="line">    l.add_xpath(<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;div[@class=product_name]&#x27;</span>)</span><br><span class="line">    l.add_xpath(<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;div[@class=product_title]&#x27;</span>)</span><br><span class="line">    l.add_xpath(<span class="string">&#x27;price&#x27;</span>, <span class="string">&#x27;p[@id=price]&#x27;</span>)</span><br><span class="line">    l.add_css(<span class="string">&#x27;stock&#x27;</span>, <span class="string">&#x27;p#stock&#x27;</span>)</span><br><span class="line">    l.add_value(<span class="string">&#x27;last_updated&#x27;</span>, <span class="string">&#x27;today&#x27;</span>)  <span class="comment"># you can also use literal values</span></span><br><span class="line">    <span class="keyword">return</span> l.load_item()</span><br></pre></td></tr></table></figure>

<p>1、loader</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.loader <span class="keyword">import</span> ItemLoader</span><br><span class="line"><span class="keyword">from</span> scrapy.loader.processors <span class="keyword">import</span> TakeFirst, Join, Compose</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NewsLoader</span>(<span class="params">ItemLoader</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    取列表非空首元素</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    default_output_processor = TakeFirst()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChainLoader</span>(<span class="params">NewsLoader</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    通过_in和_out后缀来定义输入输出处理器，合并列表成字符串，去除头尾空白字符</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    text_out = Compose(Join(), <span class="keyword">lambda</span> s: s.strip())</span><br><span class="line">    source_out = Compose(Join(), <span class="keyword">lambda</span> s: s.strip())</span><br></pre></td></tr></table></figure>

<p>2、Processor</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.loader.processors <span class="keyword">import</span> Join, MapCompose, TakeFirst</span><br><span class="line"><span class="keyword">from</span> w3lib.html <span class="keyword">import</span> remove_tags</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filter_price</span>(<span class="params">value</span>):</span></span><br><span class="line">    <span class="keyword">if</span> value.isdigit():</span><br><span class="line">        <span class="keyword">return</span> value</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Product</span>(<span class="params">scrapy.Item</span>):</span></span><br><span class="line">    name = scrapy.Field(</span><br><span class="line">        input_processor=MapCompose(remove_tags),</span><br><span class="line">        output_processor=Join(),</span><br><span class="line">    )</span><br><span class="line">    price = scrapy.Field(</span><br><span class="line">        <span class="comment"># MapCompose（能把多个函数执行的结果按顺序组合起来，产生最终的输出，通常用于输入处理器)</span></span><br><span class="line">        input_processor=MapCompose(remove_tags, filter_price),</span><br><span class="line">        output_processor=TakeFirst(),</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>

<p>3、MapCompose: MapCompose可以迭代处理一个列表输入值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.loader.processors <span class="keyword">import</span> MapCompose</span><br><span class="line">processor = MapCompose(str.upper(),<span class="keyword">lambda</span> s:s.strip())</span><br><span class="line">print(processor([<span class="string">&#x27;Hello&#x27;</span>,<span class="string">&#x27;World&#x27;</span>,<span class="string">&#x27;Python&#x27;</span>]))</span><br></pre></td></tr></table></figure>

<p>4、流程解析<br>第一步：通过add_xpath(), add_css(),add_value()方式提取数据。<br>第二步：将提取到的数据传递到输入处理器（input_processor）中，收集处理结果并保存在ItemLoader内（未分配给Item）。<br>第三步：收到所有数据后，先调用输出处理器（output_processor）来处理收集到的数据。<br>第四步：再调用load_item()方法来填充再生成Item 对象。</p>
<h2 id="补充内容"><a href="#补充内容" class="headerlink" title="补充内容"></a>补充内容</h2><p>一些参数和规则单独提取出来做成配置文件或存到数据库，可抽离的部分name、allowed_domains、start_urls等配置成Json文件。<br>rules也可以单独定义成一个python文件实现Rule分离完成配置化。<br>1、文件配置</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">	<span class="string">&quot;spider&quot;</span>:<span class="string">&quot;universal&quot;</span>,</span><br><span class="line">	<span class="string">&quot;website&quot;</span>:<span class="string">&quot;中华网科技&quot;</span>,</span><br><span class="line">	<span class="string">&quot;type&quot;</span>:<span class="string">&quot;新闻&quot;</span>,</span><br><span class="line">	<span class="string">&quot;index&quot;</span>:<span class="string">&quot;http://...&quot;</span></span><br><span class="line">	<span class="string">&quot;settings&quot;</span>:&#123;</span><br><span class="line">		<span class="string">&quot;USER_AGENT&quot;</span>:xxx</span><br><span class="line">	&#125;,</span><br><span class="line">	<span class="string">&quot;start_urls&quot;</span>:[</span><br><span class="line">		<span class="string">&quot;xxx&quot;</span></span><br><span class="line">	],</span><br><span class="line">	<span class="string">&quot;allowed_domains&quot;</span>:[</span><br><span class="line">		<span class="string">&quot;tech.china.com&quot;</span></span><br><span class="line">	],</span><br><span class="line">	<span class="string">&quot;rules&quot;</span>:<span class="string">&quot;china&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>2、读取配置</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> os.path <span class="keyword">import</span> realpath,dirname</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_config</span>(<span class="params">name</span>):</span></span><br><span class="line">	path = dirname(realpath(__file__)) + <span class="string">&#x27;/config/&#x27;</span> + name + <span class="string">&#x27;.json&#x27;</span></span><br><span class="line">	<span class="keyword">with</span> open(path,<span class="string">&#x27;r&#x27;</span>,encoding=<span class="string">&#x27;utf8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">		<span class="keyword">return</span> json.loads(f.read())</span><br></pre></td></tr></table></figure>
<p>3、启动spider</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"><span class="keyword">from</span> scrpay.utils.project import get_project_settings</span><br><span class="line"><span class="built_in">..</span>.</span><br><span class="line"></span><br><span class="line">def <span class="builtin-name">run</span>():</span><br><span class="line">	name = sys.argv[1]</span><br><span class="line">	custom_settings = get_config(name)</span><br><span class="line">	# 爬取Spider名称</span><br><span class="line">	spider = custom_settings.<span class="builtin-name">get</span>(<span class="string">&#x27;spider&#x27;</span>,<span class="string">&#x27;universal&#x27;</span>)</span><br><span class="line">	project_settings = get_project_settings()</span><br><span class="line"><span class="built_in">	settings </span>= dict(project_settings.copy())</span><br><span class="line">	# 合并配置</span><br><span class="line">	settings.update(custom_settings.<span class="builtin-name">get</span>(<span class="string">&#x27;settings&#x27;</span>))</span><br><span class="line">	process = CrawlerProcess(settings)</span><br><span class="line">	# 启动爬虫</span><br><span class="line">	process.crawl(spider,**&#123;<span class="string">&#x27;name&#x27;</span>:name&#125;)</span><br><span class="line">	process.start()</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/05/Scrapy_redis%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Leo Lin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="江湖の林九">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/08/05/Scrapy_redis%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93/" class="post-title-link" itemprop="url">Scrapy-redis基础总结</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-08-05 09:00:00" itemprop="dateCreated datePublished" datetime="2019-08-05T09:00:00+08:00">2019-08-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-04-23 17:03:21" itemprop="dateModified" datetime="2020-04-23T17:03:21+08:00">2020-04-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/" itemprop="url" rel="index"><span itemprop="name">网络爬虫</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>scrapy-redis在scrapy的架构上增加了redis，基于redis的特性扩展了如下四种组件：<br>1、Scheduler<br>2、Duplication Filter<br>3、Item Pipeline<br>4、Base Spider</p>
<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>1、将调度器的类和去重类替换为Scrapy-Redis提供的类</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 确保request存储到redis中</span></span><br><span class="line">SCHEDULER = <span class="string">&quot;scrapy_redis.scheduler.Scheduler&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 确保所有爬虫共享相同的去重指纹</span></span><br><span class="line">DUPEFILTER_CLASS = <span class="string">&quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在redis中保持scrapy-redis用到的队列，不会清除redis中的队列，从而实现暂停和恢复的功能</span></span><br><span class="line">SCHEDULER_PERSIST = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">REDIS_URL = <span class="string">&#x27;redis://:foobared@120.27.34.25:6379&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 是否在开始之前清空 调度器和去重记录，True=清空，False=不清空</span></span><br><span class="line"><span class="comment"># SCHEDULER_FLUSH_ON_START = False     </span></span><br><span class="line">             </span><br><span class="line"><span class="comment"># 去调度器中获取数据时，如果为空，最多等待时间（最后没数据，未获取到）            </span></span><br><span class="line"><span class="comment"># SCHEDULER_IDLE_BEFORE_CLOSE = 10                                    </span></span><br></pre></td></tr></table></figure>
<p>2、Pipeline配置(非必要)</p>
<figure class="highlight sas"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 将item写入<span class="meta">key</span>为 spider.name : items 的redis的<span class="meta">list</span>中</span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">　　 <span class="string">&#x27;scrapy_redis.pipelines.RedisPipeline&#x27;</span>: 100 ,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>3、修改代码：</p>
<figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">继承RedisSpider</span><br><span class="line">start_urls改为redis_key</span><br><span class="line">RedisSpider类不需要写 `allowd_domains` 和 `start_urls`</span><br><span class="line">scrapy-redis 将从在构造方法 init() 里动态定义爬虫爬取域范围，也可以选择直接写allowd_domains</span><br></pre></td></tr></table></figure>
<p>4、启动爬虫<br>爬虫程序会一直获取redis中的任务，没有任务就等待。<br>如果在redis插入了新的任务就会继续进行，之后又进入等待状态。</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lpush <span class="attr">myspider:</span>start_urls <span class="attr">https:</span><span class="comment">//www.baidu.com</span></span><br></pre></td></tr></table></figure>

<h2 id="修改请求"><a href="#修改请求" class="headerlink" title="修改请求"></a>修改请求</h2><p>RedisSpider继承于RedisMixin和Spider类，执行逻辑为RedisMixin的<code>next_requests</code>函数接收到了redis中data后，通过<code>make_request_from_data</code>函数来解码data生成url，<br><code>make_request_from_data</code>继续调用Spider类中的<code>make_requests_from_url</code>函数生成Request。<br>1、请求添加cookie参数</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def make<span class="constructor">_requests_from_url(<span class="params">self</span>, <span class="params">url</span>)</span>:</span><br><span class="line">	return <span class="constructor">Request(<span class="params">url</span>, <span class="params">dont_filter</span>=True, <span class="params">cookies</span>=<span class="params">cookies</span>, <span class="params">meta</span>=&#123;&#x27;<span class="params">cookiejar</span>&#x27;: 1&#125;)</span></span><br></pre></td></tr></table></figure>

<p>2、默认情况下Scrapy-Redis是发送GET请求获取数据的，对于某些使用POST请求的情况需要重写<code>make_request_from_data</code>函数</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def make_request_from_data(self, data):</span><br><span class="line">	<span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">	重写make_request_from_data方法，data是scrapy-redis读取redis中的[url,form_data,meta]，然后发送post请求</span></span><br><span class="line"><span class="string">	:param data: redis中都去的请求数据，是一个list</span></span><br><span class="line"><span class="string">	:return: 一个FormRequest对象</span></span><br><span class="line"><span class="string">　　&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">	data = json.loads(data)</span><br><span class="line">	url = data.<span class="builtin-name">get</span>(<span class="string">&#x27;url&#x27;</span>)</span><br><span class="line">	form_data = data.<span class="builtin-name">get</span>(<span class="string">&#x27;form_data&#x27;</span>)</span><br><span class="line">	meta = data.<span class="builtin-name">get</span>(<span class="string">&#x27;meta&#x27;</span>)</span><br><span class="line">	return FormRequest(<span class="attribute">url</span>=url, <span class="attribute">formdata</span>=form_data, <span class="attribute">meta</span>=meta, <span class="attribute">callback</span>=self.parse)</span><br></pre></td></tr></table></figure>

<h2 id="源码解读"><a href="#源码解读" class="headerlink" title="源码解读"></a>源码解读</h2><p>1、指纹生成方法</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def request<span class="constructor">_fingerprint(<span class="params">request</span>, <span class="params">include_headers</span>=None)</span>:</span><br><span class="line">    <span class="keyword">if</span> include_headers:</span><br><span class="line">        include_headers = tuple(<span class="keyword">to</span><span class="constructor">_bytes(<span class="params">h</span>.<span class="params">lower</span>()</span>)</span><br><span class="line">                                 <span class="keyword">for</span> h <span class="keyword">in</span> sorted(include_headers))</span><br><span class="line">    cache = <span class="module-access"><span class="module"><span class="identifier">_fingerprint_cache</span>.</span></span>setdefault(request, &#123;&#125;)</span><br><span class="line">    <span class="keyword">if</span> include_headers not <span class="keyword">in</span> cache:</span><br><span class="line">        fp = hashlib.sha1<span class="literal">()</span></span><br><span class="line">        fp.update(<span class="keyword">to</span><span class="constructor">_bytes(<span class="params">request</span>.<span class="params">method</span>)</span>)</span><br><span class="line">        fp.update(<span class="keyword">to</span><span class="constructor">_bytes(<span class="params">canonicalize_url</span>(<span class="params">request</span>.<span class="params">url</span>)</span>))</span><br><span class="line">        fp.update(request.body <span class="keyword">or</span> b&#x27;&#x27;)</span><br><span class="line">        <span class="keyword">if</span> include_headers:</span><br><span class="line">            <span class="keyword">for</span> hdr <span class="keyword">in</span> include_headers:</span><br><span class="line">                <span class="keyword">if</span> hdr <span class="keyword">in</span> request.headers:</span><br><span class="line">                    fp.update(hdr)</span><br><span class="line">                    <span class="keyword">for</span> v <span class="keyword">in</span> request.headers.getlist(hdr):</span><br><span class="line">                        fp.update(v)</span><br><span class="line">        cache<span class="literal">[<span class="identifier">include_headers</span>]</span> = fp.hexdigest<span class="literal">()</span></span><br><span class="line">    return cache<span class="literal">[<span class="identifier">include_headers</span>]</span></span><br></pre></td></tr></table></figure>
<p>2、调度器核心方法</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">enqueue_request</span><span class="params">(<span class="keyword">self</span>, request)</span></span><span class="symbol">:</span></span><br><span class="line">	<span class="keyword">if</span> <span class="keyword">not</span> request.dont_filter <span class="keyword">and</span> <span class="keyword">self</span>.df.request_seen(request)<span class="symbol">:</span></span><br><span class="line">		<span class="keyword">self</span>.df.log(request, <span class="keyword">self</span>.spider)</span><br><span class="line">		<span class="keyword">return</span> False</span><br><span class="line">	<span class="keyword">if</span> <span class="keyword">self</span>.<span class="symbol">stats:</span></span><br><span class="line">		<span class="keyword">self</span>.stats.inc_value(<span class="string">&#x27;scheduler/enqueued/redis&#x27;</span>, spider=<span class="keyword">self</span>.spider)</span><br><span class="line">	<span class="keyword">self</span>.queue.push(request)</span><br><span class="line">	<span class="keyword">return</span> True</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">next_request</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">	block_pop_timeout = <span class="keyword">self</span>.idle_before_close</span><br><span class="line">	request = <span class="keyword">self</span>.queue.pop(block_pop_timeout)</span><br><span class="line">	<span class="keyword">if</span> request <span class="keyword">and</span> <span class="keyword">self</span>.<span class="symbol">stats:</span></span><br><span class="line">		<span class="keyword">self</span>.stats.inc_value(<span class="string">&#x27;scheduler/dequeued/redis&#x27;</span>, spider=<span class="keyword">self</span>.spider)</span><br><span class="line">	<span class="keyword">return</span> request</span><br></pre></td></tr></table></figure>



      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/03/Scrapy%20ImagePipeline%20FilePipeline/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Leo Lin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="江湖の林九">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/08/03/Scrapy%20ImagePipeline%20FilePipeline/" class="post-title-link" itemprop="url">Scrapy ImagePipeline</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-08-03 09:00:00" itemprop="dateCreated datePublished" datetime="2019-08-03T09:00:00+08:00">2019-08-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-04-23 17:15:15" itemprop="dateModified" datetime="2020-04-23T17:15:15+08:00">2020-04-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/" itemprop="url" rel="index"><span itemprop="name">网络爬虫</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="ImagePipeline"><a href="#ImagePipeline" class="headerlink" title="ImagePipeline"></a>ImagePipeline</h2><p>1、特点：</p>
<ul>
<li>将下载图片转换成通用的JPG和RGB格式</li>
<li>避免重复下载</li>
<li>缩略图生成</li>
<li>图片大小过滤</li>
</ul>
<p>2、工作流程：</p>
<ul>
<li>在Spider中，爬取一个Item并将所需的URL放入image_urls字段中</li>
<li>从Spider返回的Item，传递到Item Pipeline中</li>
<li>项目进入ImagePipeline，image_urls组内的URLs将被Scrapy的调度器和下载器安排下载(这意味着调度器和中间件可以复用)，当优先级更高，会在其他页面被抓取前处理。项目会在这个特定的管道阶段保持”locker”的状态，直到完成图片的下载(或者由于某些原因未完成下载)。</li>
<li>图片下载完，另一个组(images)将被更新到结构中，这个组将包含一个字典列表，其中包括下载图片的信息(如下载路径、源抓取地址、图片的校验码）。images列表中的图片顺序将和源image_urls组保持一致。如果某个图片下载失败，将会记录下错误信息，图片也不会出现在images组中。</li>
</ul>
<p>3、辅助配置项</p>
<figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 针对内置ImagePipeline处理时，对URL关键字或结果关键字使用其他字段名(默认字段名image_urls,images)</span></span><br><span class="line">IMAGES_URLS_FIELD = &#x27;field_name_for_your_images_urls&#x27;</span><br><span class="line">IMAGES_RESULT_FIELD = &#x27;field_name_for_your_processed_images&#x27;</span><br><span class="line"></span><br><span class="line"><span class="comment"># ======= #</span></span><br><span class="line">IMAGES_STORE = os.path.join(os.path.abspath(os.path.dirname(__file__)), <span class="string">&quot;images&quot;</span>)  <span class="comment"># 图片存储位置</span></span><br><span class="line">IMAGES_EXPIRES = 90  <span class="comment"># 90天内抓取的都不会被重抓</span></span><br><span class="line">IMAGES_THUMBS = &#123; <span class="comment"># 生成缩略图标签及其尺寸</span></span><br><span class="line">    &#x27;small&#x27;: (50, 50),</span><br><span class="line">    &#x27;big&#x27;: (270, 270),</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 过滤小图像</span></span><br><span class="line">IMAGES_MIN_HEIGHT = 110</span><br><span class="line">IMAGES_MIN_WIDTH = 110</span><br></pre></td></tr></table></figure>
<p>4、自定义ImagePipeline</p>
<ul>
<li><code>get_media_requests()</code><br>results元组有2个元素，每个元素包含(success, <code>file_info_or_error</code>)，如下：<figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="name">True</span>,</span><br><span class="line">  &#123;<span class="symbol">&#x27;checksum</span><span class="symbol">&#x27;:</span> <span class="symbol">&#x27;2b00042f7481c7b056c4b410d28f33cf</span>&#x27;,</span><br><span class="line">   <span class="symbol">&#x27;path</span><span class="symbol">&#x27;:</span> <span class="symbol">&#x27;full/0a79c461a4062ac383dc4fade7bc09f1384a3910.jpg</span>&#x27;,</span><br><span class="line">   <span class="symbol">&#x27;url</span><span class="symbol">&#x27;:</span> <span class="symbol">&#x27;http://www.example.com/files/product1.pdf</span>&#x27;&#125;),</span><br><span class="line"> (<span class="name">False</span>,</span><br><span class="line">  Failure(<span class="name"><span class="builtin-name">...</span></span>))]</span><br></pre></td></tr></table></figure></li>
<li><code>file_path()</code><br>重写此方法可以自定义每个文件的下载路径。</li>
<li><code>item_completed()</code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.pipelines.images <span class="keyword">import</span> ImagesPipeline</span><br><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyImagesPipeline</span>(<span class="params">ImagesPipeline</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_media_requests</span>(<span class="params">self, item, info</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        将url字段取出来直接生成Request对象，加入调度队列，执行下载。</span></span><br><span class="line"><span class="string">        :param item: 生成的Item对象</span></span><br><span class="line"><span class="string">        :param info:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> image_url <span class="keyword">in</span> item[<span class="string">&#x27;image_urls&#x27;</span>]:</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(image_url)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">item_completed</span>(<span class="params">self, results, item, info</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        当单个Item完成下载，分析下载结果剔除失败图片。</span></span><br><span class="line"><span class="string">        抛出异常DropItem,该Item忽略。</span></span><br><span class="line"><span class="string">        :param results: 列表中是一个元组，第一个值是布尔值，请求成功或失败，第二个值是下载到的资源字典</span></span><br><span class="line"><span class="string">        :param item:</span></span><br><span class="line"><span class="string">        :param info:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        image_paths = [x[<span class="string">&#x27;path&#x27;</span>] <span class="keyword">for</span> ok, x <span class="keyword">in</span> results <span class="keyword">if</span> ok]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> image_paths:</span><br><span class="line">            <span class="keyword">raise</span> DropItem(<span class="string">&quot;Item contains no images&quot;</span>)</span><br><span class="line">        item[<span class="string">&#x27;image_paths&#x27;</span>] = image_paths</span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>
<h2 id="源码解读"><a href="#源码解读" class="headerlink" title="源码解读"></a>源码解读</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_media_requests</span>(<span class="params">self, item, info</span>):</span></span><br><span class="line">	<span class="keyword">return</span> [Request(x) <span class="keyword">for</span> x <span class="keyword">in</span> item.get(self.images_urls_field, [])]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">item_completed</span>(<span class="params">self, results, item, info</span>):</span></span><br><span class="line">	<span class="keyword">if</span> isinstance(item, dict) <span class="keyword">or</span> self.images_result_field <span class="keyword">in</span> item.fields:</span><br><span class="line">		item[self.images_result_field] = [x <span class="keyword">for</span> ok, x <span class="keyword">in</span> results <span class="keyword">if</span> ok]</span><br><span class="line">	<span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">file_path</span>(<span class="params">self, request, response=None, info=None</span>):</span></span><br><span class="line">	<span class="comment">## start of deprecation warning block (can be removed in the future)</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">_warn</span>():</span></span><br><span class="line">		<span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> ScrapyDeprecationWarning</span><br><span class="line">		<span class="keyword">import</span> warnings</span><br><span class="line">		warnings.warn(<span class="string">&#x27;ImagesPipeline.image_key(url) and file_key(url) methods are deprecated, &#x27;</span></span><br><span class="line">					  <span class="string">&#x27;please use file_path(request, response=None, info=None) instead&#x27;</span>,</span><br><span class="line">					  category=ScrapyDeprecationWarning, stacklevel=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">	<span class="comment"># check if called from image_key or file_key with url as first argument</span></span><br><span class="line">	<span class="keyword">if</span> <span class="keyword">not</span> isinstance(request, Request):</span><br><span class="line">		_warn()</span><br><span class="line">		url = request</span><br><span class="line">	<span class="keyword">else</span>:</span><br><span class="line">		url = request.url</span><br><span class="line"></span><br><span class="line">	<span class="comment"># detect if file_key() or image_key() methods have been overridden</span></span><br><span class="line">	<span class="keyword">if</span> <span class="keyword">not</span> hasattr(self.file_key, <span class="string">&#x27;_base&#x27;</span>):</span><br><span class="line">		_warn()</span><br><span class="line">		<span class="keyword">return</span> self.file_key(url)</span><br><span class="line">	<span class="keyword">elif</span> <span class="keyword">not</span> hasattr(self.image_key, <span class="string">&#x27;_base&#x27;</span>):</span><br><span class="line">		_warn()</span><br><span class="line">		<span class="keyword">return</span> self.image_key(url)</span><br><span class="line">	<span class="comment">## end of deprecation warning block</span></span><br><span class="line"></span><br><span class="line">	image_guid = hashlib.sha1(to_bytes(url)).hexdigest()  <span class="comment"># change to request.url after deprecation</span></span><br><span class="line">	<span class="keyword">return</span> <span class="string">&#x27;full/%s.jpg&#x27;</span> % (image_guid)</span><br></pre></td></tr></table></figure>
1、重点关注下载的实现和下载的文件命名。<br>2、内置的ImagePipeline会默认读取Item的<code>image_urls</code>字段并认为该字段是列表形式，遍历该字段然后取出每个URL进行图片下载，图片名称是以图片URL的SHA1值进行保存。<br>3、启用内部下载器:<figure class="highlight roboconf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line"> &#x27;<span class="attribute">scrapy.pipelines.files.FilesPipeline&#x27;</span>: 1,</span><br><span class="line"> &#x27;scrapy<span class="variable">.pipelines</span><span class="variable">.images</span><span class="variable">.ImagesPipeline</span>&#x27;: 2</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/03/Scrapy%20%E5%AF%B9%E6%8E%A5%E6%A8%A1%E6%8B%9F%E6%B5%8F%E8%A7%88%E5%99%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Leo Lin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="江湖の林九">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/08/03/Scrapy%20%E5%AF%B9%E6%8E%A5%E6%A8%A1%E6%8B%9F%E6%B5%8F%E8%A7%88%E5%99%A8/" class="post-title-link" itemprop="url">Scrapy 对接模拟浏览器</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-08-03 09:00:00" itemprop="dateCreated datePublished" datetime="2019-08-03T09:00:00+08:00">2019-08-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-03-28 16:53:46" itemprop="dateModified" datetime="2020-03-28T16:53:46+08:00">2020-03-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/" itemprop="url" rel="index"><span itemprop="name">网络爬虫</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="对接selenium"><a href="#对接selenium" class="headerlink" title="对接selenium"></a>对接selenium</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.http <span class="keyword">import</span> HtmlResponse</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SeleniumMiddleware</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.driver = webdriver.Chrome(<span class="string">&#x27;./chromedriver&#x27;</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span>(<span class="params">self, request, spider</span>):</span></span><br><span class="line">        <span class="keyword">if</span> spider.name == <span class="string">&#x27;seleniumSpider&#x27;</span>:</span><br><span class="line">            self.driver.get(request.url)</span><br><span class="line">            time.sleep(<span class="number">2</span>)</span><br><span class="line">            body = self.driver.page_source</span><br><span class="line">        <span class="keyword">return</span> HtmlResponse(self.driver.current_url,</span><br><span class="line">                           body=body,</span><br><span class="line">                           encoding=<span class="string">&#x27;utf-8&#x27;</span>,</span><br><span class="line">                           request=request)</span><br></pre></td></tr></table></figure>
<h2 id="对接pyppeteer"><a href="#对接pyppeteer" class="headerlink" title="对接pyppeteer"></a>对接pyppeteer</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FundscrapyDownloaderMiddleware</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="comment"># Not all methods need to be defined. If a method is not defined,</span></span><br><span class="line">    <span class="comment"># scrapy acts as if the downloader middleware does not modify the</span></span><br><span class="line">    <span class="comment"># passed objects.</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        print(<span class="string">&quot;Init downloaderMiddleware use pypputeer.&quot;</span>)</span><br><span class="line">        pyppeteer.DEBUG = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        print(os.environ.get(<span class="string">&#x27;PYPPETEER_CHROMIUM_REVISION&#x27;</span>))</span><br><span class="line">        loop = asyncio.get_event_loop()</span><br><span class="line">        task = asyncio.ensure_future(self.getbrowser())</span><br><span class="line">        loop.run_until_complete(task)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">getbrowser</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.browser = <span class="keyword">await</span> pyppeteer.launch()</span><br><span class="line">        self.page = <span class="keyword">await</span> self.browser.newPage()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">getnewpage</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">await</span> self.browser.newPage()</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span>(<span class="params">cls, crawler</span>):</span></span><br><span class="line">        <span class="comment"># This method is used by Scrapy to create your spiders.</span></span><br><span class="line">        s = cls()</span><br><span class="line">        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)</span><br><span class="line">        <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span>(<span class="params">self, request, spider</span>):</span></span><br><span class="line">        <span class="comment"># Called for each request that goes through the downloader</span></span><br><span class="line">        <span class="comment"># middleware.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Must either:</span></span><br><span class="line">        <span class="comment"># - return None: continue processing this request</span></span><br><span class="line">        <span class="comment"># - or return a Response object</span></span><br><span class="line">        <span class="comment"># - or return a Request object</span></span><br><span class="line">        <span class="comment"># - or raise IgnoreRequest: process_exception() methods of</span></span><br><span class="line">        <span class="comment">#   installed downloader middleware will be called</span></span><br><span class="line">        loop = asyncio.get_event_loop()</span><br><span class="line">        task = asyncio.ensure_future(self.usePypuppeteer(request))</span><br><span class="line">        loop.run_until_complete(task)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> HtmlResponse(url=request.url, body=task.result(), encoding=<span class="string">&quot;utf-8&quot;</span>, request=request)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">usePypuppeteer</span>(<span class="params">self, request</span>):</span></span><br><span class="line">        <span class="keyword">await</span> self.page.goto(request.url)</span><br><span class="line">        content = <span class="keyword">await</span> self.page.content()</span><br><span class="line">        <span class="keyword">return</span> content</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_response</span>(<span class="params">self, request, response, spider</span>):</span></span><br><span class="line">        <span class="comment"># Called with the response returned from the downloader.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Must either;</span></span><br><span class="line">        <span class="comment"># - return a Response object</span></span><br><span class="line">        <span class="comment"># - return a Request object</span></span><br><span class="line">        <span class="comment"># - or raise IgnoreRequest</span></span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_exception</span>(<span class="params">self, request, exception, spider</span>):</span></span><br><span class="line">        <span class="comment"># Called when a download handler or a process_request()</span></span><br><span class="line">        <span class="comment"># (from other downloader middleware) raises an exception.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Must either:</span></span><br><span class="line">        <span class="comment"># - return None: continue processing this exception</span></span><br><span class="line">        <span class="comment"># - return a Response object: stops process_exception() chain</span></span><br><span class="line">        <span class="comment"># - return a Request object: stops process_exception() chain</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spider_opened</span>(<span class="params">self, spider</span>):</span></span><br><span class="line">        spider.logger.info(<span class="string">&#x27;Spider opened: %s&#x27;</span> % spider.name)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">render</span>(<span class="params">self, url, keep_page=False</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        render page with pyppeteer</span></span><br><span class="line"><span class="string">        :param url: page url</span></span><br><span class="line"><span class="string">        :param retries: max retry times</span></span><br><span class="line"><span class="string">        :param script: js script to evaluate</span></span><br><span class="line"><span class="string">        :param wait: number of seconds to wait before loading the page, preventing timeouts</span></span><br><span class="line"><span class="string">        :param scrolldown: how many times to page down</span></span><br><span class="line"><span class="string">        :param sleep: how many long to sleep after initial render</span></span><br><span class="line"><span class="string">        :param timeout: the longest wait time, otherwise raise timeout error</span></span><br><span class="line"><span class="string">        :param keep_page: keep page not to be closed, browser object needed</span></span><br><span class="line"><span class="string">        :param browser: pyppetter browser object</span></span><br><span class="line"><span class="string">        :param with_result: return with js evaluation result</span></span><br><span class="line"><span class="string">        :return: content, [result]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># define async render</span></span><br><span class="line">        <span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">async_render</span>(<span class="params">url, keep_page</span>):</span></span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="comment"># basic render</span></span><br><span class="line">                page = <span class="keyword">await</span> self.browser.newPage()</span><br><span class="line">                <span class="keyword">await</span> asyncio.sleep(<span class="number">2</span>)</span><br><span class="line">                response = <span class="keyword">await</span> page.goto(url, options=&#123;<span class="string">&#x27;timeout&#x27;</span>: int(<span class="number">10</span> * <span class="number">1000</span>)&#125;)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> response.status != <span class="number">200</span>:</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">None</span>, <span class="literal">None</span>, response.status</span><br><span class="line">                result = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">await</span> page.waitFor(<span class="number">2000</span>)</span><br><span class="line">                <span class="comment"># get html of page</span></span><br><span class="line">                content = <span class="keyword">await</span> page.content()</span><br><span class="line">                <span class="keyword">return</span> content, result, response.status</span><br><span class="line">            <span class="keyword">except</span> TimeoutError:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">None</span>, <span class="literal">None</span>, <span class="number">500</span></span><br><span class="line">            <span class="keyword">finally</span>:</span><br><span class="line">                <span class="comment"># if keep page, do not close it</span></span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> keep_page:</span><br><span class="line">                    <span class="keyword">await</span> page.close()</span><br><span class="line"></span><br><span class="line">        content, result, status = [<span class="literal">None</span>] * <span class="number">3</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># retry for &#123;retries&#125; times</span></span><br><span class="line">        retries = <span class="number">3</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(retries):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> content:</span><br><span class="line">                content, result, status = self.loop.run_until_complete(</span><br><span class="line">                    async_render(url=url, keep_page=keep_page))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># if need to return js evaluation result</span></span><br><span class="line">        <span class="keyword">return</span> content, result, status</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/03/Scrapy%20%E6%B7%BB%E5%8A%A0%E4%BB%A3%E7%90%86%E5%92%8C%E9%9A%8F%E6%9C%BA%E8%AF%B7%E6%B1%82%E5%A4%B4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Leo Lin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="江湖の林九">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/08/03/Scrapy%20%E6%B7%BB%E5%8A%A0%E4%BB%A3%E7%90%86%E5%92%8C%E9%9A%8F%E6%9C%BA%E8%AF%B7%E6%B1%82%E5%A4%B4/" class="post-title-link" itemprop="url">Scrapy 添加代理和随机请求头</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-08-03 08:00:00" itemprop="dateCreated datePublished" datetime="2019-08-03T08:00:00+08:00">2019-08-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-04-23 17:22:47" itemprop="dateModified" datetime="2020-04-23T17:22:47+08:00">2020-04-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/" itemprop="url" rel="index"><span itemprop="name">网络爬虫</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="随机请求头"><a href="#随机请求头" class="headerlink" title="随机请求头"></a>随机请求头</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> signals</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomUserAgentMiddleware</span>(<span class="params">object</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.agent = UserAgent()</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span>(<span class="params">cls, crawler</span>):</span></span><br><span class="line">        <span class="keyword">return</span> cls()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span>(<span class="params">self, request, spider</span>):</span></span><br><span class="line">        request.headers.setdefault(<span class="string">&#x27;User-Agent&#x27;</span>, self.agent.random)</span><br><span class="line">		<span class="comment"># request.headers[&#x27;User-Agent&#x27;] = random.choice(self.user_agents)</span></span><br></pre></td></tr></table></figure>

<h2 id="添加代理"><a href="#添加代理" class="headerlink" title="添加代理"></a>添加代理</h2><p>1、对接芝麻代理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> redis</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在scrapy中使用 代理池的demo</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">middleware中代码如下</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">pool = redis.ConnectionPool(decode_responses=<span class="literal">True</span>)  <span class="comment"># redis 池</span></span><br><span class="line">r = redis.Redis(connection_pool=pool)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">middleware中配置代理中间键</span></span><br><span class="line"><span class="string">注意，根据爬取网址是http 还是https 来设置</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyProxy</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;代理IP设置&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span>(<span class="params">self, request, spider</span>):</span></span><br><span class="line">        <span class="comment"># 此处对接redis</span></span><br><span class="line">        data = r.zrangebyscore(<span class="string">&#x27;XDLProxy&#x27;</span>, <span class="number">1</span>, <span class="number">100</span>, withscores=<span class="literal">True</span>)</span><br><span class="line">        ip, score = random.choice(data)</span><br><span class="line">        request.meta[<span class="string">&#x27;proxy&#x27;</span>] = <span class="string">&#x27;http://&#x27;</span>+ip  <span class="comment"># 根据自己情况填写</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">拦截中间键中配置如下，写入计分器，满分20分</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DownloaderMiddleware</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_response</span>(<span class="params">self, request, response, spider</span>):</span></span><br><span class="line">        <span class="comment"># 对代理ip进行清洗</span></span><br><span class="line">        proxy = request._meta.get(<span class="string">&#x27;proxy&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> response.status == <span class="number">200</span>:</span><br><span class="line">            print(<span class="string">&#x27;IP访问失败&#x27;</span>)</span><br><span class="line">            <span class="keyword">if</span> proxy:</span><br><span class="line">                proxy = proxy[proxy.find(<span class="string">&#x27;/&#x27;</span>)+<span class="number">2</span>:]  <span class="comment"># 提取当此访问proxy</span></span><br><span class="line">                r.zincrby(<span class="string">&#x27;XDLProxy&#x27;</span>, <span class="number">-1</span>, proxy)  <span class="comment"># redis 命令修改</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> proxy:</span><br><span class="line">                proxy = proxy[proxy.find(<span class="string">&#x27;/&#x27;</span>) + <span class="number">2</span>:]  <span class="comment"># 提取当此访问proxy</span></span><br><span class="line">                score = r.zscore(<span class="string">&#x27;XDLProxy&#x27;</span>, proxy)  <span class="comment"># 取出分数</span></span><br><span class="line">                <span class="keyword">if</span> score &lt; <span class="number">20</span>:</span><br><span class="line">                    r.zincrby(<span class="string">&#x27;XDLProxy&#x27;</span>, <span class="number">1</span>, proxy)  <span class="comment"># redis 新版本命令更改这样了</span></span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_exception</span>(<span class="params">self, request, exception, spider</span>):</span>  <span class="comment"># 可能由于IP质量问题无法访问超时</span></span><br><span class="line">        print(<span class="string">&#x27;超时异常&#x27;</span>)</span><br><span class="line">        proxy = request._meta.get(<span class="string">&#x27;proxy&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> proxy:</span><br><span class="line">            proxy = proxy[proxy.find(<span class="string">&#x27;/&#x27;</span>) + <span class="number">2</span>:]</span><br><span class="line">            r.zincrby(<span class="string">&#x27;XDLProxy&#x27;</span>, <span class="number">-1</span>, proxy)  <span class="comment"># redis 新版本命令更改这样了</span></span><br><span class="line">            <span class="keyword">return</span> request</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">setting中配置</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">DOWNLOAD_TIMEOUT = <span class="number">5</span>  <span class="comment"># 有的时候代理ip失效，会导致一直卡在那里 ，也有可能是用http 访问https</span></span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="string">&#x27;middlewares.MyProxy&#x27;</span>: <span class="number">543</span>,  <span class="comment"># 自定义代理IP</span></span><br><span class="line">    <span class="string">&#x27;middlewares.spiderDownloaderMiddleware&#x27;</span>: <span class="number">600</span>,  <span class="comment"># 拦截301、302等跳转  必须设置到600</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>2、对接阿布云代理：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> base64</span><br><span class="line"></span><br><span class="line"><span class="comment"># 代理服务器</span></span><br><span class="line">proxyServer = <span class="string">&quot;http://http-pro.abuyun.com:9010&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 代理隧道验证信息</span></span><br><span class="line">proxyUser = <span class="string">&quot;H01234567890123P&quot;</span></span><br><span class="line">proxyPass = <span class="string">&quot;0123456789012345&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># for Python2</span></span><br><span class="line">proxyAuth = <span class="string">&quot;Basic &quot;</span> + base64.b64encode(proxyUser + <span class="string">&quot;:&quot;</span> + proxyPass)</span><br><span class="line"></span><br><span class="line"><span class="comment"># for Python3</span></span><br><span class="line"><span class="comment">#proxyAuth = &quot;Basic &quot; + base64.urlsafe_b64encode(bytes((proxyUser + &quot;:&quot; + proxyPass), &quot;ascii&quot;)).decode(&quot;utf8&quot;)</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProxyMiddleware</span>(<span class="params">object</span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">process_request</span>(<span class="params">self, request, spider</span>):</span></span><br><span class="line">		request.meta[<span class="string">&quot;proxy&quot;</span>] = proxyServer</span><br><span class="line"></span><br><span class="line">		request.headers[<span class="string">&quot;Proxy-Authorization&quot;</span>] = proxyAuth   </span><br></pre></td></tr></table></figure>
<h2 id="Cookie中间件"><a href="#Cookie中间件" class="headerlink" title="Cookie中间件"></a>Cookie中间件</h2><p>cookie池原理：<br>不断地用不同的账号登录网站，就可以得到很多不同的cookies，可以将其放在Redis里。爬虫请求网页时，可以从Redis中读取Cookies并添加。<br>例如：登录以后的Cookies转换为JSON格式的字符串并保存到Redis中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LoginMiddleware</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.client = redis.StrictRedis()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span>(<span class="params">self, request, spider</span>):</span></span><br><span class="line">        <span class="keyword">if</span> spider.name == <span class="string">&#x27;loginSpider&#x27;</span>:</span><br><span class="line">            cookies = json.loads(self.client.lpop(<span class="string">&#x27;cookies&#x27;</span>).decode())</span><br><span class="line">            request.cookies = cookies</span><br></pre></td></tr></table></figure>

<h2 id="内容补充"><a href="#内容补充" class="headerlink" title="内容补充"></a>内容补充</h2><p>1、from_crawler何时被调用?<br>Downloadmiddleware、Pipeline还有扩展EXTENSIONS的实例化Manager都是继承scrapy.middleware下的MiddlewareManager，逐个通过<code>load_object</code>方法导入模块，再调用每个模块的<code>from_crawler</code>或<code>from_settings</code>方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@classmethod</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span>(<span class="params">cls, crawler</span>):</span></span><br><span class="line">	<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">		1、用给定的参数创建了一个cls类的实例spider</span></span><br><span class="line"><span class="string">		2、用crawler实例的crawler.signals.connect方法将实例s的spider_opened方法与spider_opened信号绑定起来</span></span><br><span class="line"><span class="string">	&quot;&quot;&quot;</span></span><br><span class="line">	<span class="comment"># This method is used by Scrapy to create your spiders.</span></span><br><span class="line">	s = cls()</span><br><span class="line">	crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)</span><br><span class="line">	<span class="keyword">return</span> s</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/02/Python%20%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E4%B8%8E%E9%87%8D%E5%BB%BA%E4%BE%9D%E8%B5%96/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Leo Lin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="江湖の林九">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/08/02/Python%20%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E4%B8%8E%E9%87%8D%E5%BB%BA%E4%BE%9D%E8%B5%96/" class="post-title-link" itemprop="url">Python 虚拟环境与重建依赖</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-08-02 10:00:00" itemprop="dateCreated datePublished" datetime="2019-08-02T10:00:00+08:00">2019-08-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-04-24 09:50:00" itemprop="dateModified" datetime="2020-04-24T09:50:00+08:00">2020-04-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%BC%96%E7%A8%8B%E8%B6%B3%E8%BF%B9/" itemprop="url" rel="index"><span itemprop="name">编程足迹</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="conda"><a href="#conda" class="headerlink" title="conda"></a>conda</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">1、列出当前存在的环境</span><br><span class="line">conda <span class="builtin-name">info</span> -e </span><br><span class="line"></span><br><span class="line">2、创建新的虚拟环境</span><br><span class="line">conda create -n your_env_name <span class="attribute">python</span>=X.X</span><br><span class="line"></span><br><span class="line">3、激活环境</span><br><span class="line">activate your_env_name</span><br><span class="line"></span><br><span class="line">4、退出环境</span><br><span class="line">deactivate</span><br><span class="line"></span><br><span class="line">5、删除环境/包</span><br><span class="line">conda <span class="builtin-name">remove</span> -n your_env_name --all</span><br><span class="line">conda <span class="builtin-name">remove</span> -n your_env_name package_name（此命令针对conda install -n your_env_name package生效，对pip安装的包显示NotFound）</span><br></pre></td></tr></table></figure>
<h2 id="pipenv"><a href="#pipenv" class="headerlink" title="pipenv"></a>pipenv</h2><p>1、将pipfile，pip和virtualenv整合到一个命令中。</p>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>、查看虚拟环境路径(注需要先进入环境)</span><br><span class="line">pipenv --venv</span><br><span class="line"></span><br><span class="line"><span class="number">2</span>、查看目前安装的库及其依赖</span><br><span class="line">pipenv graph</span><br><span class="line"></span><br><span class="line"><span class="number">3</span>、创建/进入虚拟环境</span><br><span class="line">pipenv --python 指定路径或版本号 或 pipenv install</span><br><span class="line">pipenv shell <span class="comment"># 进入</span></span><br><span class="line"></span><br><span class="line"><span class="number">4</span>、退出/删除环境</span><br><span class="line"><span class="keyword">exit</span> <span class="comment"># 退出</span></span><br><span class="line">pipenv --rm <span class="comment"># 删除</span></span><br><span class="line"></span><br><span class="line"><span class="number">5</span>、在development分区安装 requests</span><br><span class="line">pipenv install requests --dev</span><br></pre></td></tr></table></figure>
<p>2、查看本机虚拟环境</p>
<figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">workon/lsvirtualenv <span class="meta"># 查看</span></span><br><span class="line">workon xxx <span class="meta"># 启用</span></span><br></pre></td></tr></table></figure>
<p>3、修改虚拟环境安装位置<br>WORKON_HOME变量指定存放目录，如下图：<br><img src="/images/env.png"></p>
<h2 id="环境依赖"><a href="#环境依赖" class="headerlink" title="环境依赖"></a>环境依赖</h2><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">freeze</span> &gt; requirements.txt</span><br><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/02/Scrapy%20Middleware%20Pipeline/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Leo Lin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="江湖の林九">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/08/02/Scrapy%20Middleware%20Pipeline/" class="post-title-link" itemprop="url">Scrapy Middleware和Pipeline</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-08-02 08:00:00" itemprop="dateCreated datePublished" datetime="2019-08-02T08:00:00+08:00">2019-08-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-03-28 16:39:49" itemprop="dateModified" datetime="2020-03-28T16:39:49+08:00">2020-03-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/" itemprop="url" rel="index"><span itemprop="name">网络爬虫</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Scrapy提供了可自定义2种中间件，1个数据处理器：<br>在爬虫的代码里面专心写数据爬取的代码；在中间件里面专心写突破反爬虫、登录、重试和渲染AJAX等操作。</p>
<table>
<thead>
<tr>
<th>名称</th>
<th>作用</th>
<th>用户设置</th>
</tr>
</thead>
<tbody><tr>
<td>数据收集器（Item-Pipeline)</td>
<td>处理item</td>
<td>覆盖</td>
</tr>
<tr>
<td>下载中间件（Downloader-Middleware）</td>
<td>处理request、response</td>
<td>合并</td>
</tr>
<tr>
<td>爬虫中间件（Spider-Middleware）</td>
<td>处理item、response、request</td>
<td>合并</td>
</tr>
</tbody></table>
<h2 id="Middleware"><a href="#Middleware" class="headerlink" title="Middleware"></a>Middleware</h2><p>数字越小代表越靠近Scrapy引擎，优先级越高会被优先调用。</p>
<figure class="highlight roboconf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES_BASE = &#123;</span><br><span class="line">    <span class="comment"># Engine side</span></span><br><span class="line">    &#x27;<span class="attribute">scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware&#x27;</span>: 100,</span><br><span class="line">    &#x27;scrapy<span class="variable">.downloadermiddlewares</span><span class="variable">.httpauth</span><span class="variable">.HttpAuthMiddleware</span>&#x27;: 300,</span><br><span class="line">    &#x27;scrapy<span class="variable">.downloadermiddlewares</span><span class="variable">.downloadtimeout</span><span class="variable">.DownloadTimeoutMiddleware</span>&#x27;: 350,</span><br><span class="line">    &#x27;scrapy<span class="variable">.downloadermiddlewares</span><span class="variable">.defaultheaders</span><span class="variable">.DefaultHeadersMiddleware</span>&#x27;: 400,</span><br><span class="line">    &#x27;scrapy<span class="variable">.downloadermiddlewares</span><span class="variable">.useragent</span><span class="variable">.UserAgentMiddleware</span>&#x27;: 500,</span><br><span class="line">    &#x27;scrapy<span class="variable">.downloadermiddlewares</span><span class="variable">.retry</span><span class="variable">.RetryMiddleware</span>&#x27;: 550,</span><br><span class="line">    &#x27;scrapy<span class="variable">.downloadermiddlewares</span><span class="variable">.ajaxcrawl</span><span class="variable">.AjaxCrawlMiddleware</span>&#x27;: 560,</span><br><span class="line">    &#x27;scrapy<span class="variable">.downloadermiddlewares</span><span class="variable">.redirect</span><span class="variable">.MetaRefreshMiddleware</span>&#x27;: 580,</span><br><span class="line">    &#x27;scrapy<span class="variable">.downloadermiddlewares</span><span class="variable">.httpcompression</span><span class="variable">.HttpCompressionMiddleware</span>&#x27;: 590,</span><br><span class="line">    &#x27;scrapy<span class="variable">.downloadermiddlewares</span><span class="variable">.redirect</span><span class="variable">.RedirectMiddleware</span>&#x27;: 600,</span><br><span class="line">    &#x27;scrapy<span class="variable">.downloadermiddlewares</span><span class="variable">.cookies</span><span class="variable">.CookiesMiddleware</span>&#x27;: 700,</span><br><span class="line">    &#x27;scrapy<span class="variable">.downloadermiddlewares</span><span class="variable">.httpproxy</span><span class="variable">.HttpProxyMiddleware</span>&#x27;: 750,</span><br><span class="line">    &#x27;scrapy<span class="variable">.downloadermiddlewares</span><span class="variable">.stats</span><span class="variable">.DownloaderStats</span>&#x27;: 850,</span><br><span class="line">    &#x27;scrapy<span class="variable">.downloadermiddlewares</span><span class="variable">.httpcache</span><span class="variable">.HttpCacheMiddleware</span>&#x27;: 900,</span><br><span class="line">    # Downloader side</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>1、Downloader Middleware<br>位置:</p>
<ul>
<li>在Scheduler调度出队列的Request发送给Downloader下载之前</li>
<li>下载后生成的Response被Spider解析之前</li>
</ul>
<p>用途：</p>
<ul>
<li>修改User-Agent\处理重定向</li>
<li>设置代理\设置Cookies</li>
<li>失败重试</li>
</ul>
<p>核心方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyMiddleware</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, spider_data</span>):</span></span><br><span class="line">        self.spider_data = spider_data</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span>(<span class="params">cls, crawler</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        获取spider的settings参数,返回中间件实例对象。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        spider_data = crawler.settings.get(<span class="string">&quot;SPIDER_DATA&quot;</span>)</span><br><span class="line">        print(<span class="string">&quot;### middleware get spider_data: &#123;&#125;&quot;</span>.format(spider_data))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> cls(spider_data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_request</span>(<span class="params">self, request, spider</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Request从队列里调度出来到Downloader下载执行之前。</span></span><br><span class="line"><span class="string">        return</span></span><br><span class="line"><span class="string">            None: 继续处理Request</span></span><br><span class="line"><span class="string">            Response: 返回Response</span></span><br><span class="line"><span class="string">            Request: 重新调度</span></span><br><span class="line"><span class="string">        raise IgnoreRequest:  process_exception -&gt; Request.errback</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        print(<span class="string">&quot;### call process_request&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_response</span>(<span class="params">self, request, response, spider</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Downloader执行下载之后得到对应的Response,发送给Spider解析之前。</span></span><br><span class="line"><span class="string">        return</span></span><br><span class="line"><span class="string">            Response: 继续处理Response</span></span><br><span class="line"><span class="string">            Request: 重新调度</span></span><br><span class="line"><span class="string">        raise IgnoreRequest: Request.errback</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        print(<span class="string">&quot;### call process_response&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_exception</span>(<span class="params">self, request, exception, spider</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        当Downloader或process_request()方法抛出异常，例如抛出IngoreRequest异常</span></span><br><span class="line"><span class="string">        return</span></span><br><span class="line"><span class="string">            None: 继续处理异常</span></span><br><span class="line"><span class="string">            Response: 返回Response</span></span><br><span class="line"><span class="string">            Request: 重新调用</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<p>2、Spider Middleware</p>
<figure class="highlight roboconf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">SPIDER_MIDDLEWARES_BASE = &#123;</span><br><span class="line">    <span class="comment"># Engine side</span></span><br><span class="line">    &#x27;<span class="attribute">scrapy.spidermiddlewares.httperror.HttpErrorMiddleware&#x27;</span>: 50,</span><br><span class="line">    &#x27;scrapy<span class="variable">.spidermiddlewares</span><span class="variable">.offsite</span><span class="variable">.OffsiteMiddleware</span>&#x27;: 500,</span><br><span class="line">    &#x27;scrapy<span class="variable">.spidermiddlewares</span><span class="variable">.referer</span><span class="variable">.RefererMiddleware</span>&#x27;: 700,</span><br><span class="line">    &#x27;scrapy<span class="variable">.spidermiddlewares</span><span class="variable">.urllength</span><span class="variable">.UrlLengthMiddleware</span>&#x27;: 800,</span><br><span class="line">    &#x27;scrapy<span class="variable">.spidermiddlewares</span><span class="variable">.depth</span><span class="variable">.DepthMiddleware</span>&#x27;: 900,</span><br><span class="line">    # Spider side</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>作用位置:</p>
<ul>
<li>Downloader生成的Response发送给Spider之前</li>
<li>Spider生成的Request发送给Scheduler之前</li>
<li>Spider生成的Item发送给Item pipeline之前</li>
</ul>
<p>核心方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpiderMiddleware</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, spider_data</span>):</span></span><br><span class="line">        self.spider_data = spider_data</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span>(<span class="params">cls, crawler</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        获取spider的settings参数,返回中间件实例对象</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        spider_data = crawler.settings.get(<span class="string">&quot;SPIDER_DATA&quot;</span>)</span><br><span class="line">        print(<span class="string">&quot;### spider middleware get spider_data: &#123;&#125;&quot;</span>.format(spider_data))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> cls(spider_data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_spider_input</span>(<span class="params">self, response, spider</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        当response被Spider Middleware处理后，马上要进入某个回调函数parse_xxx()前该方法调用</span></span><br><span class="line"><span class="string">        param</span></span><br><span class="line"><span class="string">            response:即被处理的Response</span></span><br><span class="line"><span class="string">            spider:该Response对应的Spider</span></span><br><span class="line"><span class="string">        return None  继续处理response，调用其他的Spider Middleware</span></span><br><span class="line"><span class="string">        raise Exception 调用Request的errback()方法</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        print(<span class="string">&quot;### call process_spider_input&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_spider_output</span>(<span class="params">self, response, result, spider</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        当Spider处理Response返回结果时,即yield item或者yield scrapy.Request()的时该方法被调用。</span></span><br><span class="line"><span class="string">        param</span></span><br><span class="line"><span class="string">            response：生成该输出的Response</span></span><br><span class="line"><span class="string">            result: 包含Request或Item 对象的可迭代对象即Spider返回的结果</span></span><br><span class="line"><span class="string">            spider：即其结果对应的spider</span></span><br><span class="line"><span class="string">        return</span></span><br><span class="line"><span class="string">            iterable of Request、dict or Item</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        print(<span class="string">&quot;### call process_spider_output&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> result:</span><br><span class="line">            <span class="keyword">yield</span> i</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_spider_exception</span>(<span class="params">self, response, exception, spider</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        当process_spider_input方法抛出异常时或当爬虫本身的代码出现了Exception该方法被调用。</span></span><br><span class="line"><span class="string">        param</span></span><br><span class="line"><span class="string">            response是Response对象，异常被抛出时被处理的Response</span></span><br><span class="line"><span class="string">            exception是Exception对象,即被抛出的异常</span></span><br><span class="line"><span class="string">            spider是Spider对象，即抛出该异常的Spider</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        return</span></span><br><span class="line"><span class="string">            None 继续处理该异常，调用其他Spider Middleware的process_spider_exception方法</span></span><br><span class="line"><span class="string">            iterable of Response, dict, or Item 则其他process_spider_output方法被调用</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_start_requests</span>(<span class="params">self, start_requests, spider</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        以Spider启动的Request为参数(即运行到start_requests()的时候)被调用，必须返回Request</span></span><br><span class="line"><span class="string">        param</span></span><br><span class="line"><span class="string">            start_requests:包含Request的可迭代对象</span></span><br><span class="line"><span class="string">            spider:即Start Requests所属的Spider</span></span><br><span class="line"><span class="string">        return</span></span><br><span class="line"><span class="string">            包含Request对象的可迭代对象</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p><strong>注：</strong>只需要实现其中一个核心方法就可以定义一个Spider Middleware。</p>
<h2 id="Item-Pipeline"><a href="#Item-Pipeline" class="headerlink" title="Item Pipeline"></a>Item Pipeline</h2><p>主要功能：</p>
<ul>
<li>清洗HTML数据</li>
<li>检查爬取字段</li>
<li>查重并丢弃重复内容</li>
<li>保存爬取结果</li>
</ul>
<p>核心方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MysqlPipeline</span>：</span></span><br><span class="line"><span class="class">	<span class="title">def</span> <span class="title">__init__</span>(<span class="params">...</span>):</span></span><br><span class="line">		<span class="keyword">pass</span></span><br><span class="line">		</span><br><span class="line"><span class="meta">	@classmethod</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span>(<span class="params">cls,crawler</span>):</span></span><br><span class="line">		<span class="keyword">return</span> cls(...)</span><br><span class="line">		</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">open_spider</span>(<span class="params">self,spider</span>):</span></span><br><span class="line">		<span class="keyword">pass</span></span><br><span class="line">		</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">close_spider</span>(<span class="params">self,spider</span>):</span></span><br><span class="line">		<span class="keyword">pass</span></span><br><span class="line">		</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self,item,spider</span>):</span></span><br><span class="line">		data = dict(item)</span><br><span class="line">		keys = <span class="string">&#x27;,&#x27;</span>.join(data.keys())</span><br><span class="line">		value = <span class="string">&#x27;,&#x27;</span>.join([<span class="string">&#x27;%s&#x27;</span>]*len(data))</span><br><span class="line">		sql = <span class="string">&#x27;INSERT INTO %s (%s) VALUES (%S)&#x27;</span>%(item.table,keys,values)</span><br><span class="line">		self.cursor.execute(sql,tuple(data.values()))</span><br><span class="line">		self.db.commit()</span><br><span class="line">		<span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/01/Python%20%E4%BC%98%E9%9B%85%E6%96%B9%E5%BC%8F(%E4%BC%98%E5%85%88%E7%BA%A7%E5%92%8C%E6%A0%BC%E5%BC%8F%E5%8C%96)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Leo Lin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="江湖の林九">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/08/01/Python%20%E4%BC%98%E9%9B%85%E6%96%B9%E5%BC%8F(%E4%BC%98%E5%85%88%E7%BA%A7%E5%92%8C%E6%A0%BC%E5%BC%8F%E5%8C%96)/" class="post-title-link" itemprop="url">优雅写Python(优先级和格式化)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-08-01 12:00:05" itemprop="dateCreated datePublished" datetime="2019-08-01T12:00:05+08:00">2019-08-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-04-24 09:51:40" itemprop="dateModified" datetime="2020-04-24T09:51:40+08:00">2020-04-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%BC%96%E7%A8%8B%E8%B6%B3%E8%BF%B9/" itemprop="url" rel="index"><span itemprop="name">编程足迹</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">国内源</span><br><span class="line">pip <span class="keyword">install</span> xxx -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line"></span><br><span class="line">升级单个模块</span><br><span class="line">pip <span class="keyword">install</span> <span class="comment">--upgrade xxx </span></span><br></pre></td></tr></table></figure>
<h2 id="序列解包"><a href="#序列解包" class="headerlink" title="序列解包"></a>序列解包</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">info = [<span class="string">&#x27;brucepk&#x27;</span>, <span class="string">&#x27;man&#x27;</span>, <span class="string">&#x27;python&#x27;</span>]</span><br><span class="line">name,sex,tech = info</span><br></pre></td></tr></table></figure>
<h2 id="区间判断采用链式判断"><a href="#区间判断采用链式判断" class="headerlink" title="区间判断采用链式判断"></a>区间判断采用链式判断</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>  <span class="number">80</span> &lt;= score &lt; <span class="number">90</span>:</span><br><span class="line">	level = <span class="string">&#x27;B&#x27;</span></span><br></pre></td></tr></table></figure>
<h2 id="多值条件判断"><a href="#多值条件判断" class="headerlink" title="多值条件判断"></a>多值条件判断</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> num <span class="keyword">in</span>(<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>):</span><br><span class="line">	type = <span class="string">&#x27;奇数&#x27;</span></span><br></pre></td></tr></table></figure>
<h2 id="多条件内容判断至少一个成立"><a href="#多条件内容判断至少一个成立" class="headerlink" title="多条件内容判断至少一个成立"></a>多条件内容判断至少一个成立</h2><p>常规方法<br>用 or 连接多个条件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">math,English,computer =<span class="number">90</span>,<span class="number">80</span>,<span class="number">88</span></span><br><span class="line"><span class="keyword">if</span> math&lt;<span class="number">60</span> <span class="keyword">or</span> English&lt;<span class="number">60</span> <span class="keyword">or</span> computer&lt;<span class="number">60</span>:</span><br><span class="line">	print(<span class="string">&#x27;not pass&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>优雅方法：<br>使用 any 语句。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">math,English,computer =<span class="number">90</span>,<span class="number">59</span>,<span class="number">88</span></span><br><span class="line"><span class="keyword">if</span> any([math&lt;<span class="number">60</span>,English&lt;<span class="number">60</span>,computer&lt;<span class="number">60</span>]):</span><br><span class="line">	print(<span class="string">&#x27;not pass&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="多条件判断内容全部成立"><a href="#多条件判断内容全部成立" class="headerlink" title="多条件判断内容全部成立"></a>多条件判断内容全部成立</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> all([math&gt;<span class="number">60</span>,English&gt;<span class="number">60</span>,computer&gt;<span class="number">60</span>]):</span><br><span class="line">	print(<span class="string">&#x27;pass&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="遍历序列元素和元素下标注"><a href="#遍历序列元素和元素下标注" class="headerlink" title="遍历序列元素和元素下标注"></a>遍历序列元素和元素下标注</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">L =[<span class="string">&#x27;math&#x27;</span>, <span class="string">&#x27;English&#x27;</span>, <span class="string">&#x27;computer&#x27;</span>, <span class="string">&#x27;Physics&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> k,v <span class="keyword">in</span> enumerate(L):</span><br><span class="line">	print(k, <span class="string">&#x27;:&#x27;</span>, v)</span><br></pre></td></tr></table></figure>
<h2 id="for-else语句"><a href="#for-else语句" class="headerlink" title="for/else语句"></a>for/else语句</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> mylist:</span><br><span class="line">	<span class="keyword">if</span> i == theflag:</span><br><span class="line">		<span class="keyword">break</span></span><br><span class="line">	process(i)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">	<span class="keyword">raise</span> ValueError(<span class="string">&quot;List argument missing terminal flag.&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="生成器生成费波那契数列"><a href="#生成器生成费波那契数列" class="headerlink" title="生成器生成费波那契数列"></a>生成器生成费波那契数列</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fib</span>(<span class="params">n</span>):</span></span><br><span class="line">	a, b = <span class="number">0</span>, <span class="number">1</span></span><br><span class="line">	<span class="keyword">while</span> a &lt; n:</span><br><span class="line">		<span class="keyword">yield</span> a</span><br><span class="line">		a, b = b, a + b</span><br></pre></td></tr></table></figure>
<h2 id="预设字典默认值"><a href="#预设字典默认值" class="headerlink" title="预设字典默认值"></a>预设字典默认值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data = [(<span class="string">&#x27;foo&#x27;</span>, <span class="number">10</span>), (<span class="string">&#x27;bar&#x27;</span>, <span class="number">20</span>), (<span class="string">&#x27;foo&#x27;</span>, <span class="number">39</span>), (<span class="string">&#x27;bar&#x27;</span>, <span class="number">49</span>)]</span><br><span class="line">groups = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> (key, value) <span class="keyword">in</span> data:</span><br><span class="line">	groups.setdefault(key, []).append(value)</span><br></pre></td></tr></table></figure>
<h2 id="format用法"><a href="#format用法" class="headerlink" title="format用法"></a>format用法</h2><p>1、用大括号 { } 来转义大括号</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">&quot;&#123;&#123; My &#125;&#125; int: &#123;0:d&#125;;  hex: &#123;0:#x&#125;;  oct: &#123;0:#o&#125;;  bin: &#123;0:#b&#125;  and &#123;0:&#123;label&#125;&lt;6&#125;&quot;</span>.format(<span class="number">11</span>,label=<span class="string">&#x27;#&#x27;</span>)</span><br><span class="line"><span class="string">&#x27;&#123; My &#125; int: 11;  hex: 0xb;  oct: 0o13;  bin: 0b1011  and 11####&#x27;</span></span><br></pre></td></tr></table></figure>
<p>2、可以通过key,dict,list,class形式访问参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>lists = [<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="string">&#x27;&#123;name[0]&#125; and &#123;name[1]&#125;&#x27;</span>.format(name=lists)</span><br><span class="line"><span class="string">&#x27;a and b</span></span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>原数据</th>
<th>格式</th>
<th>结果</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>3.1415926</td>
<td>{:.2f}</td>
<td>3.14</td>
<td>保留小数点后两位</td>
</tr>
<tr>
<td>3.1415926</td>
<td>{:+.2f}</td>
<td>3.14</td>
<td>带符号保留小数点后两位</td>
</tr>
<tr>
<td>-1</td>
<td>{:+.2f}</td>
<td>-1</td>
<td>带符号保留小数点后两位</td>
</tr>
<tr>
<td>2.71828</td>
<td>{:.0f}</td>
<td>3</td>
<td>不带小数</td>
</tr>
<tr>
<td>1000000</td>
<td>{:,}</td>
<td>1,000,000</td>
<td>以逗号分隔的数字格式</td>
</tr>
<tr>
<td>0.25</td>
<td>{:.2%}</td>
<td>25.00%</td>
<td>百分比格式</td>
</tr>
<tr>
<td>1000000000</td>
<td>{:.2e}</td>
<td>1.00E+09</td>
<td>指数记法</td>
</tr>
<tr>
<td>11</td>
<td>{0:b}</td>
<td>1011</td>
<td>转换成二进制</td>
</tr>
<tr>
<td>11</td>
<td>{0:d}</td>
<td>11</td>
<td>转换成十进制</td>
</tr>
<tr>
<td>11</td>
<td>{0:o}</td>
<td>13</td>
<td>转换成八进制</td>
</tr>
<tr>
<td>11</td>
<td>{0:x}</td>
<td>b</td>
<td>转换成十六进制</td>
</tr>
</tbody></table>
<h2 id="运算符优先级"><a href="#运算符优先级" class="headerlink" title="运算符优先级"></a>运算符优先级</h2><table>
<thead>
<tr>
<th>优先级</th>
<th>运算符</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>lambda</td>
<td>Lambda表达式</td>
</tr>
<tr>
<td>2</td>
<td>or</td>
<td>布尔“或”</td>
</tr>
<tr>
<td>3</td>
<td>and</td>
<td>布尔“与”</td>
</tr>
<tr>
<td>4</td>
<td>not x</td>
<td>布尔“非”</td>
</tr>
<tr>
<td>5</td>
<td>in，not in</td>
<td>成员测试</td>
</tr>
<tr>
<td>6</td>
<td>is，is not</td>
<td>同一性测试</td>
</tr>
<tr>
<td>7</td>
<td>&lt;，&lt;=，&gt;，&gt;=，!=，==</td>
<td>比较</td>
</tr>
<tr>
<td>8</td>
<td>\|</td>
<td>按位或</td>
</tr>
<tr>
<td>9</td>
<td>^</td>
<td>按位异或</td>
</tr>
<tr>
<td>10</td>
<td>&amp;</td>
<td>按位与</td>
</tr>
<tr>
<td>11</td>
<td>&lt;&lt;，&gt;&gt;</td>
<td>移位</td>
</tr>
<tr>
<td>12</td>
<td>+，-</td>
<td>加法与减法</td>
</tr>
<tr>
<td>13</td>
<td>*，/，%</td>
<td>乘法、除法与取余</td>
</tr>
<tr>
<td>14</td>
<td>+x，-x</td>
<td>正负号</td>
</tr>
<tr>
<td>15</td>
<td>~x</td>
<td>按位翻转</td>
</tr>
<tr>
<td>16</td>
<td>**</td>
<td>指数</td>
</tr>
<tr>
<td>17</td>
<td>x.attribute</td>
<td>属性参考</td>
</tr>
<tr>
<td>18</td>
<td>x[index]</td>
<td>下标</td>
</tr>
<tr>
<td>19</td>
<td>x[index:index]</td>
<td>寻址段</td>
</tr>
<tr>
<td>20</td>
<td>f(arguments...)</td>
<td>函数调用</td>
</tr>
<tr>
<td>21</td>
<td>(experession,...)</td>
<td>绑定或元组显示</td>
</tr>
<tr>
<td>22</td>
<td>[expression,...]</td>
<td>列表显示</td>
</tr>
<tr>
<td>23</td>
<td>{key:datum,...}</td>
<td>字典显示</td>
</tr>
<tr>
<td>24</td>
<td>‘expression,...‘</td>
<td>字符串转换</td>
</tr>
</tbody></table>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/01/Scrapy%20%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Leo Lin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="江湖の林九">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/08/01/Scrapy%20%E9%AB%98%E7%BA%A7%E9%85%8D%E7%BD%AE/" class="post-title-link" itemprop="url">Scrapy 高级配置</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-08-01 10:00:00" itemprop="dateCreated datePublished" datetime="2019-08-01T10:00:00+08:00">2019-08-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-05-14 19:15:37" itemprop="dateModified" datetime="2020-05-14T19:15:37+08:00">2020-05-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/" itemprop="url" rel="index"><span itemprop="name">网络爬虫</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="errback"><a href="#errback" class="headerlink" title="errback"></a>errback</h2><p>errback必须要有callback函数，failure.request就是创建的Request对象。<br>errback函数能捕获的scrapy错误有：连接建立超时，DNS错误等。</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">	<span class="keyword">for</span> u <span class="keyword">in</span> <span class="keyword">self</span>.<span class="symbol">start_urls:</span></span><br><span class="line">		<span class="keyword">yield</span> scrapy.Request(u, callback=<span class="keyword">self</span>.parse_httpbin,</span><br><span class="line">								errback=<span class="keyword">self</span>.errback_httpbin,</span><br><span class="line">								dont_filter=True)</span><br><span class="line">								</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">errback_httpbin</span><span class="params">(<span class="keyword">self</span>, failure)</span></span><span class="symbol">:</span></span><br><span class="line">	<span class="comment"># log all failures</span></span><br><span class="line">	<span class="keyword">self</span>.logger.error(repr(failure))</span><br><span class="line"></span><br><span class="line">	<span class="comment"># in case you want to do something special for some errors,</span></span><br><span class="line">	<span class="comment"># you may need the failure&#x27;s type:</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> failure.check(HttpError)<span class="symbol">:</span></span><br><span class="line">		<span class="comment"># these exceptions come from HttpError spider middleware</span></span><br><span class="line">		<span class="comment"># you can get the non-200 response</span></span><br><span class="line">		response = failure.value.response</span><br><span class="line">		<span class="keyword">self</span>.logger.error(<span class="string">&#x27;HttpError on %s&#x27;</span>, response.url)		</span><br></pre></td></tr></table></figure>

<h2 id="配置关闭条件-0代表不开启"><a href="#配置关闭条件-0代表不开启" class="headerlink" title="配置关闭条件(0代表不开启)"></a>配置关闭条件(0代表不开启)</h2><p>1、CLOSESPIDER_TIMEOUT<br>默认值：0<br>如果一个spider在指定的秒数后仍在运行， 它将以 closespider_timeout 的原因被自动关闭。</p>
<p>2、CLOSESPIDER_ITEMCOUNT<br>默认值：0<br>一个整数值，指定条目的个数。如果spider爬取条目数超过了指定的数， 并且这些条目通过item pipeline传递，spider将会以 closespider_itemcount 的原因被自动关闭。</p>
<p>3、CLOSESPIDER_PAGECOUNT<br>默认值：0<br>一个整数值，指定最大的抓取响应(reponses)数。 如果spider抓取数超过指定的值，则会以 closespider_pagecount 的原因自动关闭。</p>
<h2 id="项目相关设置"><a href="#项目相关设置" class="headerlink" title="项目相关设置"></a>项目相关设置</h2><p>1、邮件比如MAIL_FROM，可以让你配置MailSender类，该类目前用于统计邮件信息以及内存使用信息。<br>2、<code>SCRAPY_SETTINGS_MODULE</code> 以及 <code>SCRAPY_PROJECT</code>，可以调整Scrapy项目与其他项目集成的方式，比如Django。</p>
<h2 id="下载调优"><a href="#下载调优" class="headerlink" title="下载调优"></a>下载调优</h2><p>将<code>REDIRECT_PRIORITY_ADJUST</code>设为2，意味着每次发生重定向时，新请求会在所有非重定向请求完成服务后才会被调度；<br>而将<code>REDIRECT_MAX_TIMES</code>设置为20，则表示在执行20次重定向后下载器将会放弃尝试。</p>
<figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Scrapy settings for companyNews project</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta"># For simplicity, this file contains only settings considered important or</span></span><br><span class="line"><span class="meta"># commonly used. You can find more settings consulting the documentation:</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#     http://doc.scrapy.org/en/latest/topics/settings.html</span></span><br><span class="line"><span class="meta">#     http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html</span></span><br><span class="line"><span class="meta">#     http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html</span></span><br><span class="line"></span><br><span class="line">BOT_NAME = <span class="string">&#x27;companyNews&#x27;</span></span><br><span class="line"></span><br><span class="line">SPIDER_MODULES = [<span class="string">&#x27;companyNews.spiders&#x27;</span>]</span><br><span class="line">NEWSPIDER_MODULE = <span class="string">&#x27;companyNews.spiders&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#-----------------------日志文件配置-----------------------------------</span></span><br><span class="line"><span class="meta">#日志文件名</span></span><br><span class="line"><span class="meta">#LOG_FILE = &quot;dg.log&quot;</span></span><br><span class="line"><span class="meta">#日志文件级别</span></span><br><span class="line">LOG_LEVEL = <span class="string">&#x27;WARNING&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Obey robots.txt rules</span></span><br><span class="line"><span class="meta"># robots.txt 是遵循 Robot协议 的一个文件，它保存在网站的服务器中，它的作用是，告诉搜索引擎爬虫，</span></span><br><span class="line"><span class="meta"># 本网站哪些目录下的网页 不希望 你进行爬取收录。在Scrapy启动后，会在第一时间访问网站的 robots.txt 文件，</span></span><br><span class="line"><span class="meta"># 然后决定该网站的爬取范围。</span></span><br><span class="line"><span class="meta"># ROBOTSTXT_OBEY = True</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># ------------------------全局并发数的一些配置:-------------------------------</span></span><br><span class="line"><span class="meta"># Configure maximum concurrent requests performed by Scrapy (default: 16)</span></span><br><span class="line"><span class="meta"># 默认 Request 并发数：16</span></span><br><span class="line"><span class="meta"># CONCURRENT_REQUESTS = 32</span></span><br><span class="line"><span class="meta"># 默认 Item 并发数：100</span></span><br><span class="line"><span class="meta"># CONCURRENT_ITEMS = 100</span></span><br><span class="line"><span class="meta"># The download delay setting will honor only one of:</span></span><br><span class="line"><span class="meta"># 默认每个域名的并发数：16</span></span><br><span class="line"><span class="meta">#CONCURRENT_REQUESTS_PER_DOMAIN = 16</span></span><br><span class="line"><span class="meta"># 每个IP的最大并发数：0表示忽略</span></span><br><span class="line"><span class="meta"># CONCURRENT_REQUESTS_PER_IP = 0</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Configure a delay for requests for the same website (default: 0)</span></span><br><span class="line"><span class="meta"># See http://scrapy.readthedocs.org/en/latest/topics/settings.html#download-delay</span></span><br><span class="line"><span class="meta"># See also autothrottle settings and docs</span></span><br><span class="line"><span class="meta">#DOWNLOAD_DELAY 会影响 CONCURRENT_REQUESTS，不能使并发显现出来,设置下载延迟</span></span><br><span class="line"><span class="meta">#DOWNLOAD_DELAY = 3</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Disable cookies (enabled by default)</span></span><br><span class="line"><span class="meta">#禁用cookies</span></span><br><span class="line"><span class="meta"># COOKIES_ENABLED = True</span></span><br><span class="line"><span class="meta"># COOKIES_DEBUG = True</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Disable Telnet Console (enabled by default)</span></span><br><span class="line"><span class="meta">#TELNETCONSOLE_ENABLED = False</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Crawl responsibly by identifying yourself (and your website) on the user-agent</span></span><br><span class="line"><span class="meta">#USER_AGENT = &#x27;haoduofuli (+http://www.yourdomain.com)&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Override the default request headers:</span></span><br><span class="line">DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="line">  <span class="string">&#x27;Accept&#x27;</span>: <span class="string">&#x27;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;Accept-Language&#x27;</span>: <span class="string">&#x27;en&#x27;</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta"># Enable or disable spider middlewares</span></span><br><span class="line"><span class="meta"># See http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html</span></span><br><span class="line">SPIDER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="string">&#x27;companyNews.middlewares.UserAgentmiddleware&#x27;</span>: <span class="number">401</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta"># Enable or disable downloader middlewares</span></span><br><span class="line"><span class="meta"># See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html</span></span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">     <span class="string">&#x27;companyNews.middlewares.UserAgentmiddleware&#x27;</span>: <span class="number">400</span>,</span><br><span class="line">     # <span class="string">&#x27;companyNews.middlewares.CookieMiddleware&#x27;</span>: <span class="number">700</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">MYEXT_ENABLED=True      # 开启扩展</span><br><span class="line">IDLE_NUMBER=<span class="number">10</span>           # 配置空闲持续时间单位为 <span class="number">360</span>个 ，一个时间单位为<span class="number">5</span>s</span><br><span class="line"><span class="meta"># Enable or disable extensions</span></span><br><span class="line"><span class="meta"># See http://scrapy.readthedocs.org/en/latest/topics/extensions.html</span></span><br><span class="line"><span class="meta"># 在 EXTENSIONS 配置，激活扩展</span></span><br><span class="line">EXTENSIONS = &#123;</span><br><span class="line">    # <span class="string">&#x27;scrapy.extensions.telnet.TelnetConsole&#x27;</span>: None,</span><br><span class="line">    <span class="string">&#x27;companyNews.extensions.RedisSpiderSmartIdleClosedExensions&#x27;</span>: <span class="number">500</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta"># Configure item pipelines</span></span><br><span class="line"><span class="meta"># See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line"><span class="meta"># 注意:自定义pipeline的优先级需高于Redispipeline,因为RedisPipeline不会返回item,</span></span><br><span class="line"><span class="meta"># 所以如果RedisPipeline优先级高于自定义pipeline,那么自定义pipeline无法获取到item</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">     #将清除的项目在redis进行处理，# 将RedisPipeline注册到pipeline组件中(这样才能将数据存入Redis)</span><br><span class="line">    # <span class="string">&#x27;scrapy_redis.pipelines.RedisPipeline&#x27;</span>: <span class="number">400</span>,</span><br><span class="line">    <span class="string">&#x27;companyNews.pipelines.companyNewsPipeline&#x27;</span>: <span class="number">300</span>,# 自定义pipeline视情况选择性注册(可选)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta"># Enable and configure the AutoThrottle extension (disabled by default)</span></span><br><span class="line"><span class="meta"># See http://doc.scrapy.org/en/latest/topics/autothrottle.html</span></span><br><span class="line"><span class="meta">#AUTOTHROTTLE_ENABLED = True</span></span><br><span class="line"><span class="meta"># The initial download delay</span></span><br><span class="line"><span class="meta">#AUTOTHROTTLE_START_DELAY = 5</span></span><br><span class="line"><span class="meta"># The maximum download delay to be set in case of high latencies</span></span><br><span class="line"><span class="meta">#AUTOTHROTTLE_MAX_DELAY = 60</span></span><br><span class="line"><span class="meta"># The average number of requests Scrapy should be sending in parallel to</span></span><br><span class="line"><span class="meta"># each remote server</span></span><br><span class="line"><span class="meta">#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0</span></span><br><span class="line"><span class="meta"># Enable showing throttling stats for every response received:</span></span><br><span class="line"><span class="meta">#AUTOTHROTTLE_DEBUG = False</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Enable and configure HTTP caching (disabled by default)</span></span><br><span class="line"><span class="meta"># See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings</span></span><br><span class="line"><span class="meta"># ----------------scrapy默认已经自带了缓存，配置如下-----------------</span></span><br><span class="line"><span class="meta"># 打开缓存</span></span><br><span class="line"><span class="meta">#HTTPCACHE_ENABLED = True</span></span><br><span class="line"><span class="meta"># 设置缓存过期时间（单位：秒）</span></span><br><span class="line"><span class="meta">#HTTPCACHE_EXPIRATION_SECS = 0</span></span><br><span class="line"><span class="meta"># 缓存路径(默认为：.scrapy/httpcache)</span></span><br><span class="line"><span class="meta">#HTTPCACHE_DIR = &#x27;httpcache&#x27;</span></span><br><span class="line"><span class="meta"># 忽略的状态码</span></span><br><span class="line"><span class="meta">#HTTPCACHE_IGNORE_HTTP_CODES = []</span></span><br><span class="line"><span class="meta"># 缓存模式(文件缓存)</span></span><br><span class="line"><span class="meta">#HTTPCACHE_STORAGE = &#x27;scrapy.extensions.httpcache.FilesystemCacheStorage&#x27;</span></span><br><span class="line"><span class="meta">#HTTPCACHE_POLICY = &quot;scrapy.extensions.httpcache.DummyPolicy&quot; 策略：所有请求均缓存，下次请求直接访问原来的缓存即可</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#-----------------Scrapy-Redis分布式爬虫相关设置如下--------------------------</span></span><br><span class="line"><span class="meta"># Enables scheduling storing requests queue in redis.</span></span><br><span class="line"><span class="meta">#启用Redis调度存储请求队列，使用Scrapy-Redis的调度器,不再使用scrapy的调度器</span></span><br><span class="line">SCHEDULER = <span class="string">&quot;scrapy_redis.scheduler.Scheduler&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Ensure all spiders share same duplicates filter through redis.</span></span><br><span class="line"><span class="meta">#确保所有的爬虫通过Redis去重，使用Scrapy-Redis的去重组件,不再使用scrapy的去重组件</span></span><br><span class="line">DUPEFILTER_CLASS = <span class="string">&quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># 默认请求序列化使用的是pickle 但是我们可以更改为其他类似的。PS：这玩意儿2.X的可以用。3.X的不能用</span></span><br><span class="line"><span class="meta"># SCHEDULER_SERIALIZER = &quot;scrapy_redis.picklecompat&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># 使用优先级调度请求队列 （默认使用），</span></span><br><span class="line"><span class="meta"># 使用Scrapy-Redis的从请求集合中取出请求的方式,三种方式择其一即可:</span></span><br><span class="line"><span class="meta"># 分别按(1)请求的优先级/(2)队列FIFO/(先进先出)(3)栈FILO 取出请求（先进后出）</span></span><br><span class="line"><span class="meta"># SCHEDULER_QUEUE_CLASS = &#x27;scrapy_redis.queue.PriorityQueue&#x27;</span></span><br><span class="line"><span class="meta"># 可选用的其它队列</span></span><br><span class="line">SCHEDULER_QUEUE_CLASS = <span class="string">&#x27;scrapy_redis.queue.FifoQueue&#x27;</span></span><br><span class="line"><span class="meta"># SCHEDULER_QUEUE_CLASS = &#x27;scrapy_redis.queue.LifoQueue&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># Don&#x27;t cleanup redis queues, allows to pause/resume crawls.</span></span><br><span class="line"><span class="meta">#不清除Redis队列、这样可以暂停/恢复 爬取，</span></span><br><span class="line"><span class="meta"># 允许暂停,redis请求记录不会丢失(重启爬虫不会重头爬取已爬过的页面)</span></span><br><span class="line"><span class="meta">#SCHEDULER_PERSIST = True</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#----------------------redis的地址配置-------------------------------------</span></span><br><span class="line"><span class="meta"># Specify the full Redis URL for connecting (optional).</span></span><br><span class="line"><span class="meta"># If set, this takes precedence over the REDIS_HOST and REDIS_PORT settings.</span></span><br><span class="line"><span class="meta"># 指定用于连接redis的URL（可选）</span></span><br><span class="line"><span class="meta"># 如果设置此项，则此项优先级高于设置的REDIS_HOST 和 REDIS_PORT</span></span><br><span class="line"><span class="meta"># REDIS_URL = &#x27;redis://root:密码@主机ＩＰ:端口&#x27;</span></span><br><span class="line">REDIS_URL = <span class="string">&#x27;redis://root:123456@192.168.8.30:6379&#x27;</span></span><br><span class="line"><span class="meta"># 自定义的redis参数（连接超时之类的）</span></span><br><span class="line">REDIS_PARAMS=&#123;<span class="string">&#x27;db&#x27;</span>: <span class="number">2</span>&#125;</span><br><span class="line"><span class="meta"># Specify the host and port to use when connecting to Redis (optional).</span></span><br><span class="line"><span class="meta"># 指定连接到redis时使用的端口和地址（可选）</span></span><br><span class="line"><span class="meta">#REDIS_HOST = &#x27;127.0.0.1&#x27;</span></span><br><span class="line"><span class="meta">#REDIS_PORT = 6379</span></span><br><span class="line"><span class="meta">#REDIS_PASS = &#x27;19940225&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># REDIRECT_ENABLED = False</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta"># HTTPERROR_ALLOWED_CODES = [302, 301]</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta"># DEPTH_LIMIT = 3</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#------------------------------------------------------------------------------------------------</span></span><br><span class="line"><span class="meta"># 最大空闲时间防止分布式爬虫因为等待而关闭</span></span><br><span class="line"><span class="meta"># 这只有当上面设置的队列类是SpiderQueue或SpiderStack时才有效</span></span><br><span class="line"><span class="meta"># 并且当您的蜘蛛首次启动时，也可能会阻止同一时间启动（由于队列为空）</span></span><br><span class="line"><span class="meta"># SCHEDULER_IDLE_BEFORE_CLOSE = 10</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># 序列化项目管道作为redis Key存储</span></span><br><span class="line"><span class="meta"># REDIS_ITEMS_KEY = &#x27;%(spider)s:items&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># 默认使用ScrapyJSONEncoder进行项目序列化</span></span><br><span class="line"><span class="meta"># You can use any importable path to a callable object.</span></span><br><span class="line"><span class="meta"># REDIS_ITEMS_SERIALIZER = &#x27;json.dumps&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># 自定义redis客户端类</span></span><br><span class="line"><span class="meta"># REDIS_PARAMS[&#x27;redis_cls&#x27;] = &#x27;myproject.RedisClient&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># 如果为True，则使用redis的&#x27;spop&#x27;进行操作。</span></span><br><span class="line"><span class="meta"># 如果需要避免起始网址列表出现重复，这个选项非常有用。开启此选项urls必须通过sadd添加，否则会出现类型错误。</span></span><br><span class="line"><span class="meta"># REDIS_START_URLS_AS_SET = False</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># RedisSpider和RedisCrawlSpider默认 start_usls 键</span></span><br><span class="line"><span class="meta"># REDIS_START_URLS_KEY = &#x27;%(name)s:start_urls&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="meta"># 设置redis使用utf-8之外的编码</span></span><br><span class="line"><span class="meta"># REDIS_ENCODING = &#x27;latin1&#x27;</span></span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/9/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><span class="page-number current">10</span><a class="page-number" href="/page/11/">11</a><a class="extend next" rel="next" href="/page/11/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Leo Lin"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Leo Lin</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">104</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Leo Lin</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>


    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false});</script></body>
</html>
